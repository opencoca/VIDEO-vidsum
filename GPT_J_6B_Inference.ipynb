{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "GPT-J-6B Inference.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "307da73757a749bbbaa165125b853029": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a0f6417cecd04f86840c463c1c4e1237",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5b8b3269205e4bfe9a6b07314069ebcd",
              "IPY_MODEL_07a0ee88dcab44819e7ce0eb43a24088"
            ]
          }
        },
        "a0f6417cecd04f86840c463c1c4e1237": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5b8b3269205e4bfe9a6b07314069ebcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e80aa8a9bfa04a339935cd02d9961062",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1042301,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1042301,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1f62186355a146c6b13fe4d56bb9c500"
          }
        },
        "07a0ee88dcab44819e7ce0eb43a24088": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_74820b6f80684e23acb2f1f28cfa0568",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0.99M/0.99M [00:21&lt;00:00, 49.3kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0d039f4f47264aaaba69fcd08118a39e"
          }
        },
        "e80aa8a9bfa04a339935cd02d9961062": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1f62186355a146c6b13fe4d56bb9c500": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "74820b6f80684e23acb2f1f28cfa0568": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0d039f4f47264aaaba69fcd08118a39e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "03c3904f22ad416a90f869399c23e492": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_23f2e779a7b345e8b62afce051cc8f7e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_21f319640b144dcaa4678d59b1d887db",
              "IPY_MODEL_9f6821e85eee4176875d7d715ca69ff7"
            ]
          }
        },
        "23f2e779a7b345e8b62afce051cc8f7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "21f319640b144dcaa4678d59b1d887db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0900cf010eb44759bb11adef9cc499e4",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 456318,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456318,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_618d0696f48443998c61e1997fa8c36c"
          }
        },
        "9f6821e85eee4176875d7d715ca69ff7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_35d5312ec49a4531a243f6b07eb03ac5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 446k/446k [00:19&lt;00:00, 23.6kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ce19ba8d8cea43139cdb3a7067376b0f"
          }
        },
        "0900cf010eb44759bb11adef9cc499e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "618d0696f48443998c61e1997fa8c36c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "35d5312ec49a4531a243f6b07eb03ac5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ce19ba8d8cea43139cdb3a7067376b0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "54354afc0af54dc4a909a38dc0a312c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_267bbc4aec8c435b995b4a4e373fc0d8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_37643a680d1c49c3b4a2a368f1108aaf",
              "IPY_MODEL_46aeab3bdf3340748070fae298e421e7"
            ]
          }
        },
        "267bbc4aec8c435b995b4a4e373fc0d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "37643a680d1c49c3b4a2a368f1108aaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d42899b69bc54d9e9cfa5c563263b619",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1355256,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1355256,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_11253cac24cd435ca441b7054f68002f"
          }
        },
        "46aeab3bdf3340748070fae298e421e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_430b7737aa2442719cddc260bdb4a946",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.29M/1.29M [00:00&lt;00:00, 1.47MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cdfaf1a020d14b8a81aea4affa6e44fa"
          }
        },
        "d42899b69bc54d9e9cfa5c563263b619": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "11253cac24cd435ca441b7054f68002f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "430b7737aa2442719cddc260bdb4a946": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cdfaf1a020d14b8a81aea4affa6e44fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "63386f4c288240cd872851ddc906a3dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6275110257a940d39479c7fe9d2dd967",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6e7c4517ef354a4f88b1ead5624aa3f6",
              "IPY_MODEL_2c73f687ca6a468cb3762300ba6a5bc9"
            ]
          }
        },
        "6275110257a940d39479c7fe9d2dd967": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6e7c4517ef354a4f88b1ead5624aa3f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c2a7a646b1c741278fd15a40d58bd383",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 665,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 665,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_83693b9496224fdca7fc10b49abd43a4"
          }
        },
        "2c73f687ca6a468cb3762300ba6a5bc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_169348316ade472c9492a7e499b4a015",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 665/665 [00:17&lt;00:00, 37.0B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a668440c87594d7bbcf4cad1dacba8c3"
          }
        },
        "c2a7a646b1c741278fd15a40d58bd383": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "83693b9496224fdca7fc10b49abd43a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "169348316ade472c9492a7e499b4a015": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a668440c87594d7bbcf4cad1dacba8c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/opencoca/vidsum/blob/master/GPT_J_6B_Inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHIJVqHsh4An"
      },
      "source": [
        "# GPT-J-6B Inference \n",
        "\n",
        "This notebook explores how to work with the [GPT-J-6B model](https://github.com/kingoflolz/mesh-transformer-jax/#GPT-J-6B). See the link for more details about the model, including evaluation metrics and credits.\n",
        "\n",
        "Please note it takes ~10 minutes for this notebook to fully spin up. Sometimes you will runout of ram when attempting to setup the model. If this happens factory restart the runtime and try again. If it contines to happen ...\n",
        "\n",
        "Start this notebok by clicking `Runtime > Run all` or clicking `âŒ˜/Ctrl+F9`. Use `âŒ˜` on Mac and `Ctrl` on Windows and Linux. \n",
        "\n",
        "Keep an eye on this as it spins up. Google likes to make sure you are not a bot and will stop things if you ignore the \"I'm not a robot\" popup."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CMw_dSQKfhT"
      },
      "source": [
        "## Prepare it to be used\n",
        "\n",
        "### Install Dependencies\n",
        "\n",
        "First we download the model and install some dependencies. This step takes at least 5 minutes (possibly longer depending on server load).\n",
        "\n",
        "!!! **Make sure you are using a TPU runtime!** !!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdkgIa72_2FT"
      },
      "source": [
        "#@title Mount Google Drive and clone project repo\n",
        "#@markdown Not having to reinstall all libs speeds up relaunch immensely\n",
        "!git clone -q https://github.com/kingoflolz/mesh-transformer-jax.git\n",
        "\n",
        "import os, sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "nb_path = '/content/notebooks'\n",
        "os.symlink('/content/drive/My Drive/Colab Notebooks', nb_path)\n",
        "sys.path.insert(0,nb_path)\n",
        "\n",
        "pk_path = '/content/site-packages'\n",
        "os.makedirs('/content/drive/My Drive/Colab Notebooks/site-packages', exist_ok=True)\n",
        "os.symlink('/content/drive/My Drive/Colab Notebooks/site-packages', pk_path)\n",
        "sys.path.insert(0,pk_path)\n",
        "\n",
        "os.symlink('/content/notebooks/step_383500_slim.tar.zstd', '/content/step_383500_slim.tar.zstd')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7xAFw-LOYfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cfe135f-2730-40b3-ed10-fb83b5c153e5"
      },
      "source": [
        "#@title Install GPT-J-6B Network\n",
        "#@markdown So long as you already have a copy of GPT-J in your Google Drive this is a fast process. If the is is the first time it can take almost half an hour if https://the-eye.eu is flooded with traffic.\n",
        "!apt-get install -qq zstd\n",
        "\n",
        "# the \"slim\" version contain only bf16 weights and no optimizer parameters, which minimizes bandwidth and memory\n",
        "!echo \"Getting https://the-eye.eu/public/AI/GPT-J-6B/step_383500_slim.tar.zstd this can take ~6-20 min.\"\n",
        "!time wget  -nc -c https://the-eye.eu/public/AI/GPT-J-6B/step_383500_slim.tar.zstd\n",
        "\n",
        "!time tar -I zstd -xf step_383500_slim.tar.zstd\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selecting previously unselected package zstd.\n",
            "(Reading database ... 155219 files and directories currently installed.)\n",
            "Preparing to unpack .../zstd_1.3.3+dfsg-2ubuntu1.2_amd64.deb ...\n",
            "Unpacking zstd (1.3.3+dfsg-2ubuntu1.2) ...\n",
            "Setting up zstd (1.3.3+dfsg-2ubuntu1.2) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Getting https://the-eye.eu/public/AI/GPT-J-6B/step_383500_slim.tar.zstd this can take ~6-20 min.\n",
            "File â€˜step_383500_slim.tar.zstdâ€™ already there; not retrieving.\n",
            "\n",
            "\n",
            "real\t0m0.012s\n",
            "user\t0m0.002s\n",
            "sys\t0m0.004s\n",
            "\n",
            "real\t2m47.290s\n",
            "user\t0m32.622s\n",
            "sys\t0m29.936s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKUWvMaIyXJe",
        "outputId": "df52221a-560e-4bb1-c970-eedfda566ad9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@title Install Tensorflow\n",
        "#@markdown restart once this has run\n",
        "\n",
        "!pip install --target=$pk_path tensorflow==2.5.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.5.0 in /usr/local/lib/python3.7/dist-packages (2.5.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (0.4.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (2.5.0)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (2.6.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (0.2.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (0.12.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (3.1.0)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.6.3)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.34.1)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.12.1)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.1.2)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (0.37.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.12)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (3.7.4.3)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (3.17.3)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.15.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.19.5)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.1.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow==2.5.0) (1.5.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (2.25.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (0.6.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.0) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.0) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (2021.5.30)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.0) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.0) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KqQLCc244hn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97d53122-7f0d-4f73-a798-aed55fcd69b4"
      },
      "source": [
        "#@title Install GPT-J-6B Inference OpenCo's requirements.txt\n",
        "#@markdown restart once this has run\n",
        "\n",
        "!wget -O mesh-transformer-jax/requirements.txt https://gist.githubusercontent.com/opencoca/ea36cfdf091c93df337f36951fa93a50/raw/83f7e5246a0c7ed873569e1edab93ef289252f1e/requirements.txt\n",
        "\n",
        "!echo \"Quietly installing GPT-J-6B Inference OpenCo's requirements.txt\"\n",
        "!pip -q install --target=$pk_path -r mesh-transformer-jax/requirements.txt\n",
        "\n",
        "# jax 0.2.12 is required due to a regression with xmap in 0.2.13\n",
        "!pip -q install --target=$pk_path mesh-transformer-jax/ jax==0.2.12\n",
        "!pip -q install --target=$pk_path optax==0.0.9 transformers==4.12.3 dm-haiku==0.0.5 einops==0.3.2 jax==0.2.12 ray==1.8.0 \n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-05 16:26:51--  https://gist.githubusercontent.com/opencoca/ea36cfdf091c93df337f36951fa93a50/raw/83f7e5246a0c7ed873569e1edab93ef289252f1e/requirements.txt\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 414 [text/plain]\n",
            "Saving to: â€˜mesh-transformer-jax/requirements.txtâ€™\n",
            "\n",
            "\r          mesh-tran   0%[                    ]       0  --.-KB/s               \rmesh-transformer-ja 100%[===================>]     414  --.-KB/s    in 0s      \n",
            "\n",
            "2021-11-05 16:26:51 (22.8 MB/s) - â€˜mesh-transformer-jax/requirements.txtâ€™ saved [414/414]\n",
            "\n",
            "Quietly installing GPT-J-6B Inference OpenCo's requirements.txt\n",
            "  Building wheel for dm-haiku (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "  Building wheel for mesh-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YT1XmBc7BHwI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca45bcd4-094d-4777-cafd-d595b713f33e"
      },
      "source": [
        "#@title Re-Install Tensorflow\n",
        "#@markdown restart once this has run\n",
        "\n",
        "!pip install --target=$pk_path tensorflow==2.5.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.5.0\n",
            "  Using cached tensorflow-2.5.0-cp37-cp37m-manylinux2010_x86_64.whl (454.3 MB)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.12.1)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (0.2.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (3.1.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.6.3)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.19.5)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.15.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (0.4.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.12)\n",
            "Collecting tensorflow-estimator<2.6.0,>=2.5.0rc0\n",
            "  Using cached tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462 kB)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (3.3.0)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (0.12.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (3.17.3)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (0.37.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (3.7.4.3)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (2.6.0)\n",
            "Collecting grpcio~=1.34.0\n",
            "  Using cached grpcio-1.34.1-cp37-cp37m-manylinux2014_x86_64.whl (4.0 MB)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow==2.5.0) (1.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (2.25.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (1.35.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (0.6.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.0) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.0) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (0.4.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (2021.5.30)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.0) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.0) (3.6.0)\n",
            "Installing collected packages: grpcio, tensorflow-estimator, tensorflow\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.41.1\n",
            "    Uninstalling grpcio-1.41.1:\n",
            "      Successfully uninstalled grpcio-1.41.1\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.6.0\n",
            "    Uninstalling tensorflow-estimator-2.6.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.6.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.6.2\n",
            "    Uninstalling tensorflow-2.6.2:\n",
            "      Successfully uninstalled tensorflow-2.6.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-text 2.6.0 requires tensorflow<2.7,>=2.6.0, but you have tensorflow 2.5.0 which is incompatible.\n",
            "lm-eval 0.0.1 requires tensorflow-estimator==2.6.0, but you have tensorflow-estimator 2.5.0 which is incompatible.\u001b[0m\n",
            "Successfully installed grpcio-1.34.1 tensorflow-2.5.0 tensorflow-estimator-2.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO1UXepF-0Uq"
      },
      "source": [
        "## Setup Model\n",
        "\n",
        "Make sure to  restart your runtime for this will work reliably. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ex0qJgaueZtJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7d319c1-7d4b-4211-8fb8-fccaf7d01486"
      },
      "source": [
        "#@title Setup JAX for TPU use\n",
        "import os\n",
        "import requests \n",
        "from jax.config import config\n",
        "\n",
        "colab_tpu_addr = os.environ['COLAB_TPU_ADDR'].split(':')[0]\n",
        "url = f'http://{colab_tpu_addr}:8475/requestversion/tpu_driver0.1_dev20210607'\n",
        "print(url)\n",
        "requests.post(url)\n",
        "\n",
        "# The following is required to use TPU Driver as JAX's backend.\n",
        "config.FLAGS.jax_xla_backend = \"tpu_driver\"\n",
        "config.FLAGS.jax_backend_target = \"grpc://\" + os.environ['COLAB_TPU_ADDR']"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "http://10.36.203.146:8475/requestversion/tpu_driver0.1_dev20210607\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIgUVdFLe4A8"
      },
      "source": [
        "Sometimes the next step errors for some reason, just run it again Â¯\\\\\\_(ãƒ„)\\_/Â¯"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-A5IGYSaeze3"
      },
      "source": [
        "#@title Import libraries\n",
        "#@markdown If this fails reinstall OpenCo's requirments and attempt Model Setup again.\n",
        "import time\n",
        "\n",
        "import jax\n",
        "from jax.experimental import maps\n",
        "import numpy as np\n",
        "import optax\n",
        "import transformers\n",
        "\n",
        "from mesh_transformer.checkpoint import read_ckpt\n",
        "from mesh_transformer.sampling import nucleaus_sample\n",
        "from mesh_transformer.transformer_shard import CausalTransformer"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAgKq-X2kmba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213,
          "referenced_widgets": [
            "307da73757a749bbbaa165125b853029",
            "a0f6417cecd04f86840c463c1c4e1237",
            "5b8b3269205e4bfe9a6b07314069ebcd",
            "07a0ee88dcab44819e7ce0eb43a24088",
            "e80aa8a9bfa04a339935cd02d9961062",
            "1f62186355a146c6b13fe4d56bb9c500",
            "74820b6f80684e23acb2f1f28cfa0568",
            "0d039f4f47264aaaba69fcd08118a39e",
            "03c3904f22ad416a90f869399c23e492",
            "23f2e779a7b345e8b62afce051cc8f7e",
            "21f319640b144dcaa4678d59b1d887db",
            "9f6821e85eee4176875d7d715ca69ff7",
            "0900cf010eb44759bb11adef9cc499e4",
            "618d0696f48443998c61e1997fa8c36c",
            "35d5312ec49a4531a243f6b07eb03ac5",
            "ce19ba8d8cea43139cdb3a7067376b0f",
            "54354afc0af54dc4a909a38dc0a312c3",
            "267bbc4aec8c435b995b4a4e373fc0d8",
            "37643a680d1c49c3b4a2a368f1108aaf",
            "46aeab3bdf3340748070fae298e421e7",
            "d42899b69bc54d9e9cfa5c563263b619",
            "11253cac24cd435ca441b7054f68002f",
            "430b7737aa2442719cddc260bdb4a946",
            "cdfaf1a020d14b8a81aea4affa6e44fa",
            "63386f4c288240cd872851ddc906a3dc",
            "6275110257a940d39479c7fe9d2dd967",
            "6e7c4517ef354a4f88b1ead5624aa3f6",
            "2c73f687ca6a468cb3762300ba6a5bc9",
            "c2a7a646b1c741278fd15a40d58bd383",
            "83693b9496224fdca7fc10b49abd43a4",
            "169348316ade472c9492a7e499b4a015",
            "a668440c87594d7bbcf4cad1dacba8c3"
          ]
        },
        "outputId": "028f2e9c-e923-4d6c-92c1-3c48480f4a47"
      },
      "source": [
        "#@title Set Network Parameters \n",
        "params = {\n",
        "  \"layers\": 28,\n",
        "  \"d_model\": 4096,\n",
        "  \"n_heads\": 16,\n",
        "  \"n_vocab\": 50400,\n",
        "  \"norm\": \"layernorm\",\n",
        "  \"pe\": \"rotary\",\n",
        "  \"pe_rotary_dims\": 64,\n",
        "\n",
        "  \"seq\": 2048,\n",
        "  \"cores_per_replica\": 8,\n",
        "  \"per_replica_batch\": 1,\n",
        "}\n",
        "\n",
        "per_replica_batch = params[\"per_replica_batch\"]\n",
        "cores_per_replica = params[\"cores_per_replica\"]\n",
        "seq = params[\"seq\"]\n",
        "\n",
        "\n",
        "params[\"sampler\"] = nucleaus_sample\n",
        "\n",
        "# here we \"remove\" the optimizer parameters from the model (as we don't need them for inference)\n",
        "params[\"optimizer\"] = optax.scale(0)\n",
        "\n",
        "mesh_shape = (jax.device_count() // cores_per_replica, cores_per_replica)\n",
        "devices = np.array(jax.devices()).reshape(mesh_shape)\n",
        "\n",
        "maps.thread_resources.env = maps.ResourceEnv(maps.Mesh(devices, ('dp', 'mp')))\n",
        "\n",
        "tokenizer = transformers.GPT2TokenizerFast.from_pretrained('gpt2')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "307da73757a749bbbaa165125b853029",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1042301.0, style=ProgressStyle(descriptâ€¦"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "03c3904f22ad416a90f869399c23e492",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "54354afc0af54dc4a909a38dc0a312c3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1355256.0, style=ProgressStyle(descriptâ€¦"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "63386f4c288240cd872851ddc906a3dc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=665.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "T7kJZTkVQ2CZ",
        "outputId": "36f77dd6-7c50-4841-ecb4-e5beeb4b874a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@title Install Tracery \n",
        "#@markdown Tracery is a gramar for spining articles, stories, and text in general. https://github.com/aparrish/pytracery\n",
        "!pip install tracery"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tracery\n",
            "  Downloading tracery-0.1.1.tar.gz (8.1 kB)\n",
            "Building wheels for collected packages: tracery\n",
            "  Building wheel for tracery (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tracery: filename=tracery-0.1.1-py3-none-any.whl size=7696 sha256=33402efa4783c72eacc35b9f3ec3e7fb74f191fe71e01908e5822d37f2c69cf5\n",
            "  Stored in directory: /root/.cache/pip/wheels/14/38/48/da02ec3e9b648c4b52ffbdae69d9a1434e5cf621435f503486\n",
            "Successfully built tracery\n",
            "Installing collected packages: tracery\n",
            "Successfully installed tracery-0.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xgsL0IFTZt0",
        "cellView": "form"
      },
      "source": [
        "#@title Tidy things up\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwNETD2Uk8nu",
        "cellView": "form",
        "outputId": "c03b7fa2-555d-43de-db66-b6dec82e30cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "#@title Create the network\n",
        "#@markdown ...loading neural weights from the downloaded files. \n",
        "#@markdown\n",
        "#@markdown  > *This can take around 5 minutes.*\n",
        "total_batch = per_replica_batch * jax.device_count() // cores_per_replica\n",
        "\n",
        "network = CausalTransformer(params)\n",
        "\n",
        "network.state = read_ckpt(network.state, \"step_383500/\", devices.shape[1])\n",
        "\n",
        "network.state = network.move_xmap(network.state, np.zeros(cores_per_replica))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jax/experimental/maps.py:412: UserWarning: xmap is an experimental feature and probably has bugs!\n",
            "  warn(\"xmap is an experimental feature and probably has bugs!\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "key shape (8, 2)\n",
            "in shape (1, 2048)\n",
            "dp 1\n",
            "mp 8\n",
            "Total parameters: 6053381344\n",
            "read from disk/gcs in 96.2803s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-eT7Sw6if4J"
      },
      "source": [
        "# Run Model\n",
        "\n",
        "Finally, we are ready to infer with the model! The first sample takes around a minute due to compilation, but after that it should only take about 10 seconds per sample.\n",
        "\n",
        "Feel free to mess with the different sampling parameters (top_p and temp), as well as the length of the generations (gen_len, causes a recompile when initially changed - recompiles will be cached).\n",
        "\n",
        "You can also change other things like per_replica_batch in the previous cells to change how many generations are done in parallel. A larger batch has higher latency but higher throughput when measured in tokens generated/s. This is useful for doing things like best-of-n cherry picking.\n",
        "\n",
        "*Tip for best results: Make sure your prompt does not have any trailing spaces, which tend to confuse the model due to the BPE tokenization used during training.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuZq54dh8opl",
        "cellView": "form"
      },
      "source": [
        "#@markdown We allow for text wrapping of whitespace via css injection.\n",
        "\n",
        "# allow text wrapping in generated output: https://stackoverflow.com/a/61401455\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVzs2TYlvYeX",
        "cellView": "form",
        "outputId": "274ff9b7-56f5-4163-c274-97ec3ac00afb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "#@title Define our infer function. \n",
        "def infer(context, top_p=0.9, temp=1.0, gen_len=256):\n",
        "    tokens = tokenizer.encode(context)\n",
        "\n",
        "    provided_ctx = len(tokens)\n",
        "    pad_amount = seq - provided_ctx\n",
        "\n",
        "    padded_tokens = np.pad(tokens, ((pad_amount, 0),)).astype(np.uint32)\n",
        "    batched_tokens = np.array([padded_tokens] * total_batch)\n",
        "    length = np.ones(total_batch, dtype=np.uint32) * len(tokens)\n",
        "\n",
        "    start = time.time()\n",
        "    output = network.generate(batched_tokens, length, gen_len, {\"top_p\": np.ones(total_batch) * top_p, \"temp\": np.ones(total_batch) * temp})\n",
        "\n",
        "    samples = []\n",
        "    decoded_tokens = output[1][0]\n",
        "\n",
        "    for o in decoded_tokens[:, :, 0]:\n",
        "      samples.append(f\"\\033[1m{context}\\033[0m{tokenizer.decode(o)}\")\n",
        "\n",
        "    #print(f\"completion done in {time.time() - start:06}s\")\n",
        "    #this should instead be added to the object returned infered.samples and infered.completion_time\n",
        "    return samples\n",
        "\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjWw1zkEOP8z",
        "cellView": "form",
        "outputId": "6a6283c6-9bcd-4163-bc87-5bc927d0ac03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#@title Warmup our inference engine \n",
        "#@markdown The first time that infrences are run for a given length takes imensly longer than on repeated runs. For this reason we spin up the inference engine with a few sample runs. With this out of the way we can quickly experiment with our infrence engine. \n",
        "\n",
        "context = \"\"\"\n",
        "# A NEW WAY TO WORK IN THE OLD PORT OF MONTREAL\n",
        "## SPACES CITÃ‰ MULTIMÃ‰DIA\n",
        "\n",
        "The Old Port of Montreal is a\"\"\"\n",
        "\n",
        "for i in [32,64,96,128,256,512,1024,2048]:\n",
        "  print(\"- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\")\n",
        "  print(f\" ## Infering context with a gen_len of {i} this should take ~{i/24} sec. when run again ##\")\n",
        "  print(\"- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\")\n",
        "  print(infer(context, gen_len=i)[0])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            " ## Infering context with a gen_len of 32 this should take ~1.3333333333333333 sec. when run again ##\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "\u001b[1m\n",
            "# A NEW WAY TO WORK IN THE OLD PORT OF MONTREAL\n",
            "## SPACES CITÃ‰ MULTIMÃ‰DIA\n",
            "\n",
            "The Old Port of Montreal is a\u001b[0m complex of unique institutions that has always been an important part of the downtown scene. Long used as an entrance to the city, its vibrancy has increased with\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            " ## Infering context with a gen_len of 64 this should take ~2.6666666666666665 sec. when run again ##\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "\u001b[1m\n",
            "# A NEW WAY TO WORK IN THE OLD PORT OF MONTREAL\n",
            "## SPACES CITÃ‰ MULTIMÃ‰DIA\n",
            "\n",
            "The Old Port of Montreal is a\u001b[0m traditional commercial hub, built at the bottom of the St. Lawrence River. This unique place is a mixed environment with attractive sights and an industrial heritage from the past. Spaces CitÃ© MultimÃ©dia proposes an artistic and professional event that brings together the greatest minds in this part of the city. The Workshops\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            " ## Infering context with a gen_len of 96 this should take ~4.0 sec. when run again ##\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "\u001b[1m\n",
            "# A NEW WAY TO WORK IN THE OLD PORT OF MONTREAL\n",
            "## SPACES CITÃ‰ MULTIMÃ‰DIA\n",
            "\n",
            "The Old Port of Montreal is a\u001b[0m place of constant cultural renovation. It has evolved from a manufacturing centre of docks and wharfs in the late 19th century, to a cultural hub in the mid-twentieth century, to an alternative art and cultural centre for the 21st century. New changes in how people access the port are enhancing this dynamic new space. Using a range of tools including industrial archaeology, oral history, place-based research, environmental assessments, economic studies and virtual reality, Spaces Cit\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            " ## Infering context with a gen_len of 128 this should take ~5.333333333333333 sec. when run again ##\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "\u001b[1m\n",
            "# A NEW WAY TO WORK IN THE OLD PORT OF MONTREAL\n",
            "## SPACES CITÃ‰ MULTIMÃ‰DIA\n",
            "\n",
            "The Old Port of Montreal is a\u001b[0m major tourist attraction and the heart of the French Canadian community. Nowhere else in North America is there such a concentration of cultural and intellectual activity in one place. As a centre of music, art, fashion, commerce, community organizations and more, the Old Port is still one of the most fascinating areas of Montreal.\n",
            "\n",
            "### BUILDING A BETTER, BICYCLIST-FRIENDLY PORT\n",
            "\n",
            "Buildings are being demolished. Decks are being redone. The seagulls are hanging around again. In summer, the climate is getting warmer, and more people are venturing out from behind\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            " ## Infering context with a gen_len of 256 this should take ~10.666666666666666 sec. when run again ##\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "\u001b[1m\n",
            "# A NEW WAY TO WORK IN THE OLD PORT OF MONTREAL\n",
            "## SPACES CITÃ‰ MULTIMÃ‰DIA\n",
            "\n",
            "The Old Port of Montreal is a\u001b[0m magnet for people of all ages, a fact that many people will recognize right away. This has been known for many years and various studies have been done, among which those of Professor Maxime Gauvin, of McGill University, appear in this book and have been referenced here.\n",
            "\n",
            "Another study was conducted in 2007 by a team of statisticians from the UniversitÃ© du QuÃ©bec Ã  MontrÃ©al, led by Gilles Imbert. A majority of the population lived in the old town and an average density of 2,418 people per square kilometre is projected to be reached in 2027.\n",
            "\n",
            "In 2004, University of Toronto researchers tested the hypothesis that immigrants from eastern Europe would find their way to Montreal's city centre more readily than the residents of that country, and found that this was indeed the case.\n",
            "\n",
            "In 2012, University of Montreal researchers studied 1,035 inhabitants of Montreal to compare it to the results of their 2011 study. Their conclusion? \"Montreal is losing its ability to host people of all nationalities and ethnic backgrounds.\"\n",
            "\n",
            "â€¢ The Old Port is a working harbour in the heart of the city. Between 1859 and 1873, the old city took shape and the residential population (mostly French Canadians) was concentrated here and it attracted\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            " ## Infering context with a gen_len of 512 this should take ~21.333333333333332 sec. when run again ##\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "\u001b[1m\n",
            "# A NEW WAY TO WORK IN THE OLD PORT OF MONTREAL\n",
            "## SPACES CITÃ‰ MULTIMÃ‰DIA\n",
            "\n",
            "The Old Port of Montreal is a\u001b[0m dynamic place that is always in motion. This dynamism expresses itself through a diversified array of spaces that encourage change, renewal and new encounters. It is in this complex environment that a multitude of buildings make a formidably multilayered fabric. The most striking of these include the Gothic stone buildings from the 19th century. The most notable is the HÃ´tel de Ville, which took its inspiration from the original city council house. Densely populated by elegant businesses, this classic structure was long the vanguard of the city's development. It was the location of a number of social and cultural meetings, as well as the home of many banks, insurance companies, real estate agencies and service organizations. Most importantly, however, it was the landmark in the most imposing square in Montreal, with a signpost by the PÃ©riphÃ©rique. Today, the City of Montreal and its partners are transforming it into a space where numerous creative initiatives can be witnessed in the downtown core of the city. This is a symbol of the continued dynamism of the Old Port as it seeks to adapt to the new challenges facing the city.\n",
            "\n",
            "One of the first buildings in the Old Port is the former terminus of the city's first railroad, which was inaugurated on December 8, 1845. The Pavilion Dorion of 1875 and the ThÃ©Ã¢tre Dorion of 1887 stand across the square in front of the HÃ´tel de Ville. The Decauville (1889) and the Bourse du Commerce (1893) form the axis of the Place Royal, which, when it was a zoo, was a great attraction for the public. The Convention Centre, inaugurated in 1996, symbolizes the renewal and revival of the centre of the city, while its immense form gives a sense of solidity and permanence to the Old Port of Montreal.\n",
            "\n",
            "F.D.R. and Le Congress of Vienna\n",
            "\n",
            "The original residences of Henri-Alexandre Taschereau and Henri-Gustave Joly de LotbiniÃ¨re were converted into the convention center in 1956. It was then called the Exposition Coloniale. The main hall, called the CanopÃ©e, is modeled on an African _beni_ (or meeting place), characterized by the flat roofs resting on massive stone columns supporting the high ceilings. In 1976, the faÃ§ade facing the Saint-Laurent was turned into a gigantic monumental entrance, now named the Champ de Mars.\n",
            "\n",
            "Since then\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            " ## Infering context with a gen_len of 1024 this should take ~42.666666666666664 sec. when run again ##\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "\u001b[1m\n",
            "# A NEW WAY TO WORK IN THE OLD PORT OF MONTREAL\n",
            "## SPACES CITÃ‰ MULTIMÃ‰DIA\n",
            "\n",
            "The Old Port of Montreal is a\u001b[0m hotbed of cultural activity. The city of Montreal is home to over 200 different cultural institutions, from the biggest and most traditionalâ€”the Bell Centre and the Quartier des Spectacles, and the National Libraryâ€”to the newest, the newly built Centre D'exposition Du Sud-Ouest (in Brossard, just east of Montreal) that opened its doors in 2013. In the Old Port, you can experience cutting-edge arts, international films and vaudeville shows. You can even learn how to cut hair, paint your own tattoo, dance the hula hoop, and ride a Segway!\n",
            "\n",
            "These are just some of the activities that you can enjoy in the Old Port. Whether you are looking to escape the bustle of the modern world or just want to walk around town and make a quick stop to pick up your lunch, the Old Port is waiting for you.\n",
            "\n",
            "## Old Port |   \n",
            "---|---\n",
            "\n",
            "The Old Port of Montreal is also home to more than 15,000 artists and is the only place in Canada to host Canada's largest \"city\" of theatre. After all, the Arts Olympiques du QuÃ©bec (AOQ) is located in the centre of the Old Port (1881 rue du Petit Champlain, 4th floor), where you can find activities, exhibitions and workshops.\n",
            "\n",
            "Outside of the AOQ, there are many options, including live performances on stage, museums, art galleries and nightclubs. \n",
            "\n",
            "## Contents\n",
            "\n",
            "Planning\n",
            "\n",
            "Expectations & Festivals\n",
            "\n",
            "Exploring the Old Port\n",
            "\n",
            "Shopping\n",
            "\n",
            "Outdoor Activities\n",
            "\n",
            "Where to Eat & Stay\n",
            "\n",
            "Transport\n",
            "\n",
            "Montreal | New York\n",
            "\n",
            "|\n",
            "\n",
            "**Features**\n",
            "\n",
            "**Montreal City Guide**\n",
            "\n",
            "**When to Go**\n",
            "\n",
            "**An Insider's Guide to**\n",
            "\n",
            "**Lonely Planet**\n",
            "Planning\n",
            "\n",
            "Planning your trip\n",
            "\n",
            "Where to stay\n",
            "\n",
            "**Where to Eat**\n",
            "\n",
            "**Where to See**\n",
            "\n",
            "**Top Day Trips from Montreal**\n",
            "\n",
            "**Montreal's Performing Arts Scene**\n",
            "\n",
            "**Culture**\n",
            "\n",
            "**Shopping**\n",
            "\n",
            "**Entertainment**\n",
            "\n",
            "**Festivals**\n",
            "\n",
            "**Shopping**\n",
            "\n",
            "Festivals and events\n",
            "\n",
            "Festival du Voyageur\n",
            "\n",
            "Festival d'Ã©tÃ© de la chanson franÃ§aise\n",
            "\n",
            "Festival d'Ã©tÃ© du spectacle\n",
            "\n",
            "Festival international des films documentaires de MontrÃ©al\n",
            "\n",
            "Festival international de l'humour noir\n",
            "\n",
            "Festival international du film francophone\n",
            "\n",
            "Festival Off\n",
            "\n",
            "**Winterfest**\n",
            "\n",
            "Festival international de jazz de MontrÃ©al\n",
            "\n",
            "Festival Off\n",
            "\n",
            "Winter Jazz\n",
            "\n",
            "**What to Buy**\n",
            "\n",
            "Montreal | New York City\n",
            "\n",
            "|\n",
            "\n",
            "**What to Buy**\n",
            "\n",
            "**Shop Owners**\n",
            "\n",
            "**Currency**\n",
            "\n",
            "**How to Exchange Money**\n",
            "\n",
            "**Hours of Operation**\n",
            "\n",
            "**Entry Requirements**\n",
            "\n",
            "**Valuables**\n",
            "\n",
            "**Visas**\n",
            "\n",
            "**Money & Costs**\n",
            "\n",
            "**ATMs**\n",
            "\n",
            "**Bank Hours**\n",
            "\n",
            "**Banking Locations**\n",
            "\n",
            "**Exchange Rate**\n",
            "\n",
            "**Consumer Ratings**\n",
            "\n",
            "**Insurance**\n",
            "\n",
            "**Telephone**\n",
            "\n",
            "**Tipping**\n",
            "\n",
            "**Time**\n",
            "\n",
            "**Transport**\n",
            "\n",
            "**Getting There**\n",
            "\n",
            "**Getting Around**\n",
            "\n",
            "**Tickets & Tours**\n",
            "\n",
            "**Getting to the Airport**\n",
            "\n",
            "**Airport Transfer**\n",
            "\n",
            "**Public Transport**\n",
            "\n",
            "**Getting To the Port of Montreal**\n",
            "\n",
            "Getting to and from the Port of Montreal by car\n",
            "\n",
            "Getting to and from the Port of Montreal by train\n",
            "\n",
            "Getting to and from the Port of Montreal by bus\n",
            "\n",
            "Getting to and from the Port of Montreal by ferry\n",
            "\n",
            "Getting to and from the Port of Montreal by bike\n",
            "\n",
            "Getting to and from the Port of Montreal by taxi\n",
            "\n",
            "Driving around Montreal\n",
            "\n",
            "Riding around Montreal\n",
            "\n",
            "Getting Around Montreal\n",
            "\n",
            "By foot\n",
            "\n",
            "By public transportation\n",
            "\n",
            "By bicycle\n",
            "\n",
            "By taxi\n",
            "\n",
            "By car\n",
            "\n",
            "By boat\n",
            "\n",
            "By bus\n",
            "\n",
            "Tickets & tours\n",
            "\n",
            "Train\n",
            "\n",
            "Public Transit\n",
            "\n",
            "Taxi\n",
            "\n",
            "By boat\n",
            "\n",
            "By car\n",
            "\n",
            "Tickets\n",
            "\n",
            "**Guided Tours**\n",
            "\n",
            "**Tours**\n",
            "\n",
            "Montreal by bike\n",
            "\n",
            "Montreal's Best Walking Tours\n",
            "\n",
            "**Expectations & Festivals**\n",
            "\n",
            "**Safety**\n",
            "\n",
            "**What to Bring**\n",
            "\n",
            "**What to Wear**\n",
            "\n",
            "**General Tips**\n",
            "\n",
            "**Women Travelers**\n",
            "\n",
            "**Carrying Cash**\n",
            "\n",
            "**Children**\n",
            "\n",
            "**What to Eat**\n",
            "\n",
            "**Beaches**\n",
            "\n",
            "**Where to Eat**\n",
            "\n",
            "**Where to\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            " ## Infering context with a gen_len of 2048 this should take ~85.33333333333333 sec. when run again ##\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "\u001b[1m\n",
            "# A NEW WAY TO WORK IN THE OLD PORT OF MONTREAL\n",
            "## SPACES CITÃ‰ MULTIMÃ‰DIA\n",
            "\n",
            "The Old Port of Montreal is a\u001b[0m living example of a region where cultural, economic, and architectural shifts coincide with one another and where traditional practices and industries are slowly losing ground to new realities. Space has long been a topic of interest in the city of Montreal, especially its distinct spatial configurations and organization. What is new about these research works is their ability to address old and new questions in the fields of urban economics and space planning in a period when the balance between former trends is beginning to shift toward more prosumer and sustainable models.\n",
            "\n",
            "We often encounter the idea that space is about _mise en abÃ®me_, about writing on the horizon. This has been a favorite metaphor of Canadian architecture since the 1950s, when Le Corbusier and the Bauhaus studios started using it to describe the relationship between the past and the future. So-called new city movements, in particular the 1960s, 1970s, and 1980s, tended to forget about old Montreal and its remarkable but often forgotten spatial character. But an increasing interest in the urban built environment has enabled scholars to take a closer look at the history of the Old Port of Montreal in an effort to clarify the relationship between traditional urban structures, such as the warehouse district of the Pointe-Ã -CalliÃ¨re neighborhood, the streets of St-Paul, and the office districts of Mount Royal and Ville-Marie.\n",
            "\n",
            "How is Montreal changing today? More and more people are starting to live, work, and spend time in the Old Port, sometimes in districts that have been neglected for more than fifty years. Public policy supports these activities by granting various community organizations special tax rates. A significant proportion of the district's inhabitants reside in low-income housing or transitional housing, and many also live and work in other neighborhoods of the city, especially in the north and west of Montreal. The importance of Montreal's port as a global transport hub has also drawn many people there to settle and thrive. New actors have appeared on the scene: artists, businesses, and entrepreneurs looking for places to practice their art and find sources of inspiration.\n",
            "\n",
            "The research conducted at the Centre de recherche en Ã©tudes urbaines sur les centres-sud-est (CURES), as well as at the Institut de recherches urbaines et architecturales (IURBA), clearly shows that Old Montreal has undergone a certain spatial reconfiguration. Urban researchers are, therefore, asking a series of questions about the Old Port's physical condition and its \"mind-set.\" Although the district's physical condition has not changed dramatically, the role of the Old Port as a center for socioeconomic, cultural, and institutional growth and change has changed in some important respects. New public policies, new entrepreneurs, and new urban inhabitants are challenging old conventions that were built to suit a previous generation of residents and were never designed to cope with the needs of twenty-first-century urban residents. Furthermore, even though most of the old industrial and commercial structures of the Old Port have been preserved and incorporated into the administrative and commercial heart of Montreal, its institutional functions have been diffused throughout the entire city. The capacity to direct the economic, institutional, and social energies that flow in and out of the port has been decentralized.\n",
            "\n",
            "Building more port facilities, opening new museums and parks, installing and renovating key infrastructure, and creating housing for low-income and senior citizens are all urban strategies designed to foster greater interactions between the private and public sectors. These plans for the future are designed to create a greater density of the areas close to the port, hoping that increasing density will attract new economic activity and increase overall urban wealth.\n",
            "\n",
            "The Old Port of Montreal will always be important for many reasons. First, because of its long and distinguished history; second, because of its relationship to the port, which brings a large amount of money to the city every year; and finally, because it is a \"centre-sud\" that houses many of the key players in the city's life. The creation of such a social and cultural hub is a matter of necessity and politics; it is not an intellectual exercise.\n",
            "\n",
            "AndrÃ© Charpentier and Lionel Paquet-Helmer\n",
            "\n",
            "## **1  \n",
            " _Where are we coming from?_**\n",
            "\n",
            "**MONTRÃ‰ALERIES**\n",
            "\n",
            "**Table 1.1** | **_MontrÃ©al au XIXe siÃ¨cle_** | **_MontrÃ©al: 1880â€“1929_**\n",
            "\n",
            "---|---|---\n",
            "\n",
            "**1930** | |\n",
            "\n",
            "**1931** | |\n",
            "\n",
            "**1932** | |\n",
            "\n",
            "**1933** | |\n",
            "\n",
            "**1934** | |\n",
            "\n",
            "**1935** | |\n",
            "\n",
            "**1936** | |\n",
            "\n",
            "**1937** | |\n",
            "\n",
            "**1938** | |\n",
            "\n",
            "**1939** | |\n",
            "\n",
            "**1940** | |\n",
            "\n",
            "**1941** | |\n",
            "\n",
            "**1942** | |\n",
            "\n",
            "**1943** | |\n",
            "\n",
            "**1944** | |\n",
            "\n",
            "**1945** | |\n",
            "\n",
            "**1946** | |\n",
            "\n",
            "**1947** | |\n",
            "\n",
            "**1948** | |\n",
            "\n",
            "**1949** | |\n",
            "\n",
            "**1950** | |\n",
            "\n",
            "**1951** | |\n",
            "\n",
            "**1952** | |\n",
            "\n",
            "**1953** | |\n",
            "\n",
            "**1954** | |\n",
            "\n",
            "**1955** | |\n",
            "\n",
            "**1956** | |\n",
            "\n",
            "**1957** | |\n",
            "\n",
            "**1958** | |\n",
            "\n",
            "**1959** | |\n",
            "\n",
            "**1960** | |\n",
            "\n",
            "**1961** | |\n",
            "\n",
            "**1962** | |\n",
            "\n",
            "**1963** | |\n",
            "\n",
            "**1964** | |\n",
            "\n",
            "**1965** | |\n",
            "\n",
            "**1966** | |\n",
            "\n",
            "**1967** | |\n",
            "\n",
            "**1968** | |\n",
            "\n",
            "**1969** | |\n",
            "\n",
            "**1970** | |\n",
            "\n",
            "**1971** | |\n",
            "\n",
            "**1972** | |\n",
            "\n",
            "**1973** | |\n",
            "\n",
            "**1974** | |\n",
            "\n",
            "**1975** | |\n",
            "\n",
            "**1976** | |\n",
            "\n",
            "**1977** | |\n",
            "\n",
            "**1978** | |\n",
            "\n",
            "**1979** | |\n",
            "\n",
            "**1980** | |\n",
            "\n",
            "**1981** | |\n",
            "\n",
            "**1982** | |\n",
            "\n",
            "**1983** | |\n",
            "\n",
            "**1984** | |\n",
            "\n",
            "**1985** | |\n",
            "\n",
            "**1986** | |\n",
            "\n",
            "**1987** | |\n",
            "\n",
            "**1988** | |\n",
            "\n",
            "**1989** | |\n",
            "\n",
            "**1990** | |\n",
            "\n",
            "**1991** | |\n",
            "\n",
            "**1992** | |\n",
            "\n",
            "**1993** | |\n",
            "\n",
            "**1994** | |\n",
            "\n",
            "**1995** | |\n",
            "\n",
            "**1996** | |\n",
            "\n",
            "**1997** | |\n",
            "\n",
            "**1998** | |\n",
            "\n",
            "**1999** | |\n",
            "\n",
            "**2000** | |\n",
            "\n",
            "**2001** | |\n",
            "\n",
            "**2002** | |\n",
            "\n",
            "**2003** | |\n",
            "\n",
            "**2004** | |\n",
            "\n",
            "**2005** | |\n",
            "\n",
            "**2006** | |\n",
            "\n",
            "**2007** | |\n",
            "\n",
            "**2008** | |\n",
            "\n",
            "**2009** | |\n",
            "\n",
            "**2010** | |\n",
            "\n",
            "**2011** | |\n",
            "\n",
            "**2012** | |\n",
            "\n",
            "**2013** | |\n",
            "\n",
            "**2014** | |\n",
            "\n",
            "**2015** | |\n",
            "\n",
            "**2016** | |\n",
            "\n",
            "**1701** | |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mO89-sKMrucR"
      },
      "source": [
        "# Example prompts\n",
        "\n",
        "We've compiled the model. At this point it should only take about 15 seconds per sample. The first time you run inference on a new dataset takes longer than usual because of compilation to build the graph and write data into disk buffers for fast access by CPUs/GPUs.\n",
        "\n",
        "The following parameters can be adjusted: top_p, temp, gen_len. When changing the length of generations (gen_len), recompilation is required to maintain accuracy and consistency in results.\n",
        "\n",
        "You can also change other things like the number of generations done in parallel and how many samples are generated. In addition, you can increase or decrease the batch size for latency/throughput tradeoffs as well. This is useful when performing best-of-n cherry picking tasks where throughput needs to be maximized with minimum batch time delay on each sample generation process.\n",
        "\n",
        "To get the best results, make sure your prompt does not have any trailing spaces. This is because BPE tokenization can be confused by them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-ViSfFkOUbS"
      },
      "source": [
        "## Simple Inference "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EStwU8WqXnBD"
      },
      "source": [
        "### Set Sampling Parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akM-tZFTXnBE",
        "cellView": "form"
      },
      "source": [
        "#@markdown ### Leave this alone unless you know what you are doing! \n",
        "#@markdown ... or you are just having fun experimenting and don't mind having weird results.\n",
        "#@markdown - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
        "#@markdown Top_p controls diversity/quality. The higher our top_p the more rich and nuanced oir inferences can be.\n",
        "top_p = 0.9 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "#@markdown - - - \n",
        "#@markdown Temp controls randomnes. Low temps make for mechanical inferences.\n",
        "temp = 0.9 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "#@markdown - - - \n",
        "#@markdown Gen_len adjects the length of text infered.\n",
        "#@markdown Specifcily it adustes the nember of tokens (~4 per engish word).\n",
        "#@markdown\n",
        "#@markdown *Note: Changing gen_len will cause long wait times when first setting lengs that have not been set before.*\n",
        "#@markdown  \n",
        "gen_len = 64 #@param {type: \"slider\", min:64, max: 1024, step: 64}\n",
        "#@markdown - - - \n",
        "#@markdown Generate controls the number of inferences that will be generated.\n",
        "generate = 1 #@param {type: \"slider\", min:1, max: 12, step: 1}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wUL-ryfYueS"
      },
      "source": [
        "### Text examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvlAK6RbCJYg",
        "cellView": "form"
      },
      "source": [
        "#@title Raw Context \n",
        "#@markdown This is an example of simple summery infrence.\n",
        "\n",
        "#print(\"Enter/Paste your content. Ctrl-D or Ctrl-Z ( windows ) to save it.\")\n",
        "#context = []\n",
        "#while True:\n",
        "#    try:\n",
        "#        line = input()\n",
        "#    except EOFError:\n",
        "#        break\n",
        "#    context.append(line)\n",
        "\n",
        "context = \"\"\"\"\n",
        "\n",
        "Article:\n",
        "\n",
        "tâ€™s an intense crypto dogfight as dogecoin (DOGE) and shiba inu (SHIB), two of the most popular meme coins, battle it out for the ninth spot on the list of top digital assets by market capitalization. Some traders are profiting from the action by taking spread trades.\n",
        "\n",
        "DOGE has come alive after lagging SHIB by a significant margin earlier this month. The joke cryptocurrency surged to $0.335 on Coinbase on Thursday, hitting the highest level since Aug. 20. It was last trading near $0.30, representing a 22% gain on the day.\n",
        "\n",
        "Meanwhile, SHIB crashed as much as 30% to $0.00006 earlier today, having chalked out a rally to $0.00009 in the seven days to Oct. 27.\n",
        "\n",
        "Summery:\"\"\"\n",
        "\n",
        "print(infer(top_p=top_p, temp=1, gen_len=96, context=context)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2omRWxDivro",
        "cellView": "form"
      },
      "source": [
        "#@title Expand into a script \n",
        "#@markdown The will expand a lead into a script bassed on the opening of pulp fiction.\n",
        "write_script_on_this = \"Feline biology is amazing\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown - - - - - - - - - - - - - - - - - - - - - - - - -\n",
        "#@markdown These options don't need to be adjusted.\n",
        "top_p = 0.9 #@ param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "temp = 0.9 #@ param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "gen_len = 512 #@param {type: \"slider\", min:64, max: 2048, step: 64}\n",
        "generate = 1 #@ param {type: \"slider\", min:1, max: 12, step: 1}\n",
        "#@markdown - - - - - - - - - - - - - - - - - - - - - - - - -\n",
        "\n",
        "\n",
        "context = \"\"\"\n",
        "\n",
        "INT. COMPUTER LAB â€“ MORNING\n",
        "\n",
        "A normal computer lab, an office or university, Toronto Canada.\n",
        "It's about 9:00 in the morning. While the place isn't jammed,\n",
        "there's a healthy number of people working, drinking coffee, \n",
        "reading at their computers.\n",
        "\n",
        "Two of these people are a YOUNG MAN and a YOUNG WOMAN. The\n",
        "Young Man has a slight working-class English accent.\n",
        "\n",
        "             YOUNG MAN\n",
        "  I want to tell you about what we've been studying.\n",
        "\n",
        "             YOUNG WOMAN\n",
        "  Oh? What have you been focusing on? \n",
        "\n",
        "             YOUNG MAN\n",
        "  {expand_this}. For example\"\"\"\n",
        "\n",
        "\n",
        "for x in range(generate):\n",
        "  try:\n",
        "    print(infer(top_p=top_p, \n",
        "                temp=temp, \n",
        "                gen_len=gen_len, \n",
        "                context=context.format(expand_this = expand_this))[0])\n",
        "  except:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eN8f4mvA97nG",
        "cellView": "form"
      },
      "source": [
        "#@title Make a title sound more academic \n",
        "#@markdown  \n",
        "improve_this = \"we're going ti make an instrument using the potentiometer \" #@param {type:\"string\"}\n",
        "#@markdown - - -\n",
        "top_p = 0.9 #@ param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "temp = 0.9 #@ param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "gen_len = 64 #@ param {type: \"slider\", min:64, max: 2048, step: 64}\n",
        "generate = 12 # @ param {type: \"slider\", min:1, max: 12, step: 1}\n",
        "\n",
        "context = \"\"\"\n",
        "Instead of saying, \"{improve_this}\" a better shibboleth would be, \\\"\"\"\"\n",
        "\n",
        "#Indie Hackers want to click on titles such as \n",
        "#use something with more punch\n",
        "\n",
        "for x in range(generate):\n",
        "  try:\n",
        "    print(infer(top_p=top_p, \n",
        "                temp=temp, \n",
        "                gen_len=gen_len, \n",
        "                context=context.format(improve_this = improve_this))[0].split('\"')[3])\n",
        "  except:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrfIoe5BFaNq"
      },
      "source": [
        "str_list = [None, '', 0, \"Hi\", ' ', \"Hello\"]\n",
        "[x for x in str_list if x != '']\n",
        "[None, 0, \"Hi\", \"Hello\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vf63EvRmipH2"
      },
      "source": [
        "#@title Give headlines some more Clickbait \n",
        "improve_this = \"Make an instrument using the a light sensor \" #@param {type:\"string\"}\n",
        "#@markdown - - - - - - - - - - - - - - - - - - - - - - - - \n",
        "top_p = 0.9 #@ param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "temp = 1 #@ param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "gen_len = 64 # @ param {type: \"slider\", min:64, max: 2048, step: 64}\n",
        "generate = 5 #@ param {type: \"slider\", min:1, max: 12, step: 1}\n",
        "\n",
        "context = f\"\"\"\n",
        "You need to make your title more clickbate. So instead of,\n",
        "\"{improve_this}\" use something with more punch such as,\\\"\"\"\"\n",
        "\n",
        "import codecs\n",
        "\n",
        "def yield_quoted_text(top_p=top_p, \n",
        "                       temp=temp, \n",
        "                       gen_len=gen_len, \n",
        "                       context=context):\n",
        "  while True:\n",
        "    inference = infer(top_p=top_p, \n",
        "                       temp=temp, \n",
        "                       gen_len=gen_len, \n",
        "                       context=context)[0].split('\"')[3].strip()\n",
        "    inference = repr(inference).split('\\\\x1b[0m')\n",
        "    inference = inference[1]\n",
        "    if inference !=\"'\":\n",
        "      yield inference\n",
        "\n",
        "#for x in range(generate):\n",
        "#    while not got_a_headline:\n",
        "for x in range(generate):\n",
        "  headline = next(yield_quoted_text())\n",
        "  print(headline)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCjGKMEJlDjJ"
      },
      "source": [
        "# @ title Generating inference prompts using tracery\n",
        "import tracery\n",
        "from tracery.modifiers import base_english\n",
        "\n",
        "rules = {\n",
        "    \n",
        "    'origin': 'It was a #type_of_day# #time# in #location# and',\n",
        "    'time': ['night', 'day', 'week', 'year'],\n",
        "    'type_of_day': ['amazing', 'spectaculare' , 'horid', 'spooky' ],\n",
        "    'location': ['Montreal', 'the woods', 'my head']\n",
        "}\n",
        "\n",
        "grammar = tracery.Grammar(rules)\n",
        "grammar.add_modifiers(base_english)\n",
        "\n",
        "for x in range(generate):\n",
        "  try:\n",
        "    print(infer(top_p=top_p, \n",
        "                temp=temp, \n",
        "                gen_len=gen_len, \n",
        "                context=grammar.flatten(\"#origin#\"))[0] + \"\\n\")\n",
        "  except:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvek96CyOnOh"
      },
      "source": [
        "# @ title Generating inference prompts using tracery\n",
        "import tracery\n",
        "from tracery.modifiers import base_english\n",
        "\n",
        "rules = {\n",
        "    \n",
        "    'origin': 'Our team builds #imagine# new online #location# beyond',\n",
        "    'imagine': ['amazing', 'spectaculare' , 'bold', ],\n",
        "    'location': ['worlds', 'spaces', 'colabortion']\n",
        "}\n",
        "\n",
        "grammar = tracery.Grammar(rules)\n",
        "grammar.add_modifiers(base_english)\n",
        "\n",
        "for x in range(generate):\n",
        "  try:\n",
        "    print(infer(top_p=top_p, \n",
        "                temp=temp, \n",
        "                gen_len=gen_len, \n",
        "                context=grammar.flatten(\"#origin#\"))[0] + \"\\n\")\n",
        "  except:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5cJmv5ALxaM"
      },
      "source": [
        "#@title Generating grading comment inference prompts using tracery\n",
        "import tracery\n",
        "from tracery.modifiers import base_english\n",
        "\n",
        "rules = {\n",
        "    \n",
        "    'origin': '#this_semester# Jane #does# #good_bad# in class. We hope that she',\n",
        "    'this_semester': ['Our focus this semester was learning to program microcomputers, using sensors, and building electronic circuits.','This semester we focused on learning to build electric circuits, reading information from sensors, and programming microcontrollers.'],\n",
        "    'does': ['works', 'does' , 'particpates', 'performs' ],\n",
        "    'good_bad': ['well', 'competently', 'baddly']\n",
        "}\n",
        "\n",
        "grammar = tracery.Grammar(rules)\n",
        "grammar.add_modifiers(base_english)\n",
        "\n",
        "for x in range(generate):\n",
        "  try:\n",
        "    print(infer(top_p=top_p, \n",
        "                temp=temp, \n",
        "                gen_len=gen_len, \n",
        "                context=grammar.flatten(\"#origin#\"))[0] + \"\\n\")\n",
        "  except:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c9FzqkVcZyZ",
        "cellView": "form"
      },
      "source": [
        "#@title Article Generator \n",
        "title = \"Being your own Boss\" #@param {type:\"string\"}\n",
        "site = \"\" #@param {type:\"string\"}\n",
        "date = \"\" #@param {type:\"string\"}\n",
        "author = \"Jim Rohn\" #@param {type:\"string\"}\n",
        "tags = \"goals, drive, gtd\" #@param {type:\"string\"}\n",
        "lead = \"## Table of Content\" #@param {type:\"string\"}\n",
        "#@markdown - - -\n",
        "generate = 1\n",
        "\n",
        "context = \"\"\"\n",
        "# {title}\n",
        "### Published:{date}\n",
        "### By:{author}\n",
        "##### Tags: {tags}\n",
        "### Site: {site}\n",
        "\n",
        "{lead}\n",
        "\"\"\"\n",
        "\n",
        "for x in range(generate):\n",
        "  print(infer(top_p=top_p, \n",
        "              temp=temp, \n",
        "              gen_len=gen_len, \n",
        "              context=context.format(title =title, \n",
        "                                     site = site, \n",
        "                                     date = date, \n",
        "                                     author = author, \n",
        "                                     tags = tags, \n",
        "                                     lead = lead))[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "nKpUf-M554f9"
      },
      "source": [
        "#@title Article generation { form-width: \"300px\" }\n",
        "title = \"COVID-19: What do vaccine passports in Quebec & Ontario mean for Canada's largest provinces? \" #@param {type:\"string\"}\n",
        "site = \"https://www.canada.ca/en/public-health/services/diseases/\" #@param {type:\"string\"}\n",
        "date = \"Sep 7, 2021\" #@param {type:\"string\"}\n",
        "author = \"\" #@param {type:\"string\"}\n",
        "tags = \"COVID-19, Quebec, Ontario, Canada\" #@param {type:\"string\"}\n",
        "lead = \"The passports are certificates that confirm vaccinations and allow people to enjoy eating\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "#print(\"Enter/Paste your content. Ctrl-D or Ctrl-Z ( windows ) to save it.\")\n",
        "#context = []\n",
        "#while True:\n",
        "#    try:\n",
        "#        line = input()\n",
        "#    except EOFError:\n",
        "#        break\n",
        "#    context.append(line)\n",
        "\n",
        "context = \"\"\"\n",
        "# {title}\n",
        "### Published:{date}\n",
        "### By:{author}\n",
        "##### Tags: {tags}\n",
        "### Site: {site}\n",
        "\n",
        "{lead}\"\"\"\n",
        "\n",
        "for x in range(generate):\n",
        "  print(infer(top_p=top_p, \n",
        "              temp=temp, \n",
        "              gen_len=gen_len, \n",
        "              context=context.format(title =title, \n",
        "                                     site = site, \n",
        "                                     date = date, \n",
        "                                     author = author, \n",
        "                                     tags = tags, \n",
        "                                     lead = lead))[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka8vrZ-kN1Qi"
      },
      "source": [
        "## Code Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiXYadumWbT9"
      },
      "source": [
        "### Set Sampling Parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSKHAVCYPt2W",
        "cellView": "form"
      },
      "source": [
        "#@markdown ### Leave this alone unless you know what you are doing ... or are just having fun experimenting.\n",
        "#@markdown \n",
        "#@markdown - - - \n",
        "#@markdown Top_p controls diversity/quality. The higher our top_p the more rich and nuanced oir inferences can be.\n",
        "top_p = 0.9 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "#@markdown - - - \n",
        "#@markdown Temp controls randomnes. Low temps make for mechanical inferences.\n",
        "temp = 0.9 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "#@markdown - - - \n",
        "#@markdown Gen_len adjects the length of text infered.\n",
        "#@markdown Specifcily it adustes the nember of tokens (~4 per engish word).\n",
        "#@markdown\n",
        "#@markdown *Note: Changing gen_len will cause long wait times when first setting lengs that have not been set before.*\n",
        "#@markdown  \n",
        "gen_len = 512 #@param {type: \"slider\", min:64, max: 1024, step: 64}\n",
        "#@markdown - - - \n",
        "#@markdown Generate controls the number of inferences that will be generated.\n",
        "generate = 1 #@param {type: \"slider\", min:1, max: 12, step: 1}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIGNc1D7XBQ-"
      },
      "source": [
        "### Python Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "phNf7ZPj3I8U"
      },
      "source": [
        "#@title Python Code - Simple \n",
        "#@markdown Generate code by just writng a comment and a name for a fuction \n",
        "python_comment = \"check if a number is prime or not\" #@param {type:\"string\"}\n",
        "#variable_n_val = \"youtube_video_url\" #@param {type:\"string\"}\n",
        "function_name = \"p\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "context = \"\"\"# {python_comment}\n",
        "\n",
        "def is_\"\"\".format(python_comment = python_comment, \n",
        "           function_name = function_name)\n",
        "\n",
        "for x in range(generate):\n",
        "  print(infer(top_p=top_p, temp=temp, gen_len=gen_len, context=context)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Gs97bc_DCNIY"
      },
      "source": [
        "#@title Python Code - Complex\n",
        "python_comment = \"detect security flaws \" #@param {type:\"string\"}\n",
        "variable_name =\"person_real_name\" #@param {type:\"string\"}\n",
        "variable_val = \"Isabelle Plante\" #@param {type:\"string\"}\n",
        "function_name = \"online_account_scan\" #@param {type:\"string\"}\n",
        "#@markdown - - -\n",
        "\n",
        "\n",
        "context = '''#!/usr/bin/env python3\n",
        "\n",
        "# {python_comment}\n",
        "\n",
        "{variable_name} = \"{variable_val}\"\n",
        "\n",
        "def {function_name}(n):\n",
        "  \"\"\"This'''.format(python_comment = python_comment,\n",
        "           variable_name = variable_name,\n",
        "           variable_val = variable_val,\n",
        "           function_name = function_name)\n",
        "\n",
        "for x in range(generate):\n",
        "  print(infer(top_p=top_p, temp=temp, gen_len=gen_len, context=context)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2u9WMX9Q-lG"
      },
      "source": [
        "#@title Vue Code - Simple { form-width: \"500px\" }\n",
        "comment = \"guest speakers\" #@param {type:\"string\"}\n",
        "#variable_n_val = \"youtube_video_url\" #@param {type:\"string\"}\n",
        "name = \"tab_1\" #@param {type:\"string\"}\n",
        "#@markdown - - - \n",
        "#@markdown Leave this alone unless you know ...\n",
        "top_p = 0.8 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "temp = 0.9 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "gen_len = 512 #@param {type:\"slider\", min:64, max:1024, step:64}\n",
        "generate = 1 #@param {type: \"slider\", min:1, max: 12, step: 1}\n",
        "\n",
        "context = \"\"\"\n",
        "<!-- Vue template for {comment} page -->\n",
        "<section>\n",
        "  <div id=\"app\" class=\"container\">\n",
        "    <div class=\"content\">\n",
        "      <div id=\"{name} \" class=\"\"\".format(comment = comment, \n",
        "           name = name)\n",
        "      \n",
        "print(infer(top_p=top_p, temp=temp, gen_len=gen_len, context=context)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZruXX6EYejiF"
      },
      "source": [
        "### JSON Gen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vb8SkRpDm0rP",
        "outputId": "34c5263f-0651-478e-9187-e65cc5c268fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "#@title Json Code - Simple { form-width: \"500px\" }\n",
        "title = \"eventSponsors\" #@param {type:\"string\"}\n",
        "#variable_n_val = \"youtube_video_url\" #@param {type:\"string\"}\n",
        "name_1 = \"logoImage\" #@param {type:\"string\"}\n",
        "#@markdown - - - \n",
        "#@markdown Leave this alone unless you know ...\n",
        "top_p = 0.9 # @ param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "temp = 0.9 # @ param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "gen_len = 128 # @ param {type:\"slider\", min:64, max:1024, step:64}\n",
        "generate = 1 # @ param {type: \"slider\", min:1, max: 12, step: 1}\n",
        "\n",
        "context = f\"\"\"{{\"{title}\": [\n",
        "  {{\"{name_1}\":\"\"\"\n",
        "\n",
        "inference = infer(top_p=top_p, temp=temp, gen_len=gen_len, context=context)[0]\n",
        "print(inference)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m{\"eventSponsors\": [\n",
            "  {\"logoImage\":\u001b[0m \"http://assets.seattleairport.com/about/images/logos/AirportLogo.png\", \"position\": \"Airlines\", \"thumbnailImage\": \"http://assets.seattleairport.com/about/images/logos/AirportLogo.png\", \"company\": \"Airports\", \"imageUrl\": \"http://assets.seattleairport.com/about/images/logos/AirportLogo.png\", \"logo\": \"http://assets.seattleairport.com/about/images/logos/AirportLog\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "l5KKmPmApfXF"
      },
      "source": [
        "#@title Extend last infrence\n",
        "inference = infer(\n",
        "    top_p=top_p, \n",
        "    temp=temp, \n",
        "    gen_len=1024, \n",
        "    context=inference)[0]\n",
        "print(inference)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xiwQxqCNdz9"
      },
      "source": [
        "# Examples of GPT genereated code.\n",
        "\n",
        "The following are examples of code generated in prior runs.\n",
        "\n",
        "Please paste interesting examples below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgb9xlP394_5"
      },
      "source": [
        "# check if a number is prime or not\n",
        "\n",
        "def is_prime(n):\n",
        "  \"\"\" \n",
        "  > returns true if n is a prime,\n",
        "  > false otherwise.\n",
        "  \"\"\"\n",
        "  for i in range(2, n):\n",
        "    if n % i == 0:\n",
        "      return False\n",
        "  return True\n",
        "\n",
        "print(is_prime(7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKo_2bvagu2K"
      },
      "source": [
        "# Program to check if a number is prime or not\n",
        "\n",
        "num = 9\n",
        "\n",
        "# Program to check if a number is prime or not\n",
        "\n",
        "def is_prime(n):\n",
        "    while n > 1:\n",
        "        while n % 2 == 0:\n",
        "            return False\n",
        "        n = n / 2\n",
        "    return True\n",
        "\n",
        "print(\"This is an example of a function that works but is quite wrong!\")\n",
        "\n",
        "for i in range(2, 20):\n",
        "  print(f\"Is {i} prime? {is_prime(i)}\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOwWzOgflhW0"
      },
      "source": [
        "# check if a number is prime or not\n",
        "def is_prime(n):\n",
        "    for i in range(2, n):\n",
        "        if n % i == 0:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "is_prime(9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TA5qvTVnS0q8"
      },
      "source": [
        "!pip install grip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXng0F4dSi4i"
      },
      "source": [
        "!pip install grip\n",
        "from google.colab.output import eval_js\n",
        "print(eval_js(\"google.colab.kernel.proxyPort(8000)\"))\n",
        "!grip 8000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Shgf_1CZSkI1"
      },
      "source": [
        "!pip install flask"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}