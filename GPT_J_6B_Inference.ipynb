{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "GPT-J-6B Inference.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/opencoca/vidsum/blob/master/GPT_J_6B_Inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHIJVqHsh4An"
      },
      "source": [
        "# GPT-J-6B Inference \n",
        "\n",
        "This notebook explores how to work with the [GPT-J-6B model](https://github.com/kingoflolz/mesh-transformer-jax/#GPT-J-6B). See the link for more details about the model, including evaluation metrics and credits.\n",
        "\n",
        "Please note it takes ~10 minutes for this notebook to fully spin up. Sometimes you will runout of ram when attempting to setup the model. If this happens factory restart the runtime and try again. If it contines to happen ...\n",
        "\n",
        "Start this notebok by clicking `Runtime > Run all` or clicking `⌘/Ctrl+F9`. Use `⌘` on Mac and `Ctrl` on Windows and Linux. \n",
        "\n",
        "Keep an eye on this as it spins up. Google likes to make sure you are not a bot and will stop things if you ignore the \"I'm not a robot\" popup."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CMw_dSQKfhT"
      },
      "source": [
        "## Prepare it to be used\n",
        "\n",
        "### Install Dependencies\n",
        "\n",
        "First we download the model and install some dependencies. This step takes at least 5 minutes (possibly longer depending on server load).\n",
        "\n",
        "!!! **Make sure you are using a TPU runtime!** !!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdkgIa72_2FT"
      },
      "source": [
        "#@title Mount Google Drive and install libs to Google Drive\n",
        "#@markdown Not having to reinstall all libs speeds up relaunch immensely\n",
        "\n",
        "import os, sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "nb_path = '/content/notebooks'\n",
        "os.symlink('/content/drive/My Drive/Colab Notebooks', nb_path)\n",
        "sys.path.insert(0,nb_path)\n",
        "\n",
        "pk_path = '/content/site-packages'\n",
        "os.makedirs('/content/drive/My Drive/Colab Notebooks/site-packages', exist_ok=True)\n",
        "os.symlink('/content/drive/My Drive/Colab Notebooks/site-packages', pk_path)\n",
        "sys.path.insert(0,pk_path)\n",
        "\n",
        "os.symlink('/content/drive/My Drive/Colab Notebooks/step_383500_slim.tar.zstd', 'content/step_383500_slim.tar.zstd')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YT1XmBc7BHwI"
      },
      "source": [
        "#@title Install Tensorflow\n",
        "\n",
        "!pip install --target=$pk_path tensorflow==2.5.0\n",
        "exit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7xAFw-LOYfe"
      },
      "source": [
        "#@title Install GPT-J-6B Network\n",
        "!apt install -qq zstd\n",
        "\n",
        "# the \"slim\" version contain only bf16 weights and no optimizer parameters, which minimizes bandwidth and memory\n",
        "!echo \"Getting https://the-eye.eu/public/AI/GPT-J-6B/step_383500_slim.tar.zstd this can take ~6-20 min.\"\n",
        "!time wget -c https://the-eye.eu/public/AI/GPT-J-6B/step_383500_slim.tar.zstd\n",
        "\n",
        "!time tar -I zstd -xf step_383500_slim.tar.zstd\n",
        "\n",
        "!git clone https://github.com/kingoflolz/mesh-transformer-jax.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KqQLCc244hn"
      },
      "source": [
        "#@title Install GPT-J-6B Inference OpenCo's requirements.txt\n",
        "\n",
        "!wget -O mesh-transformer-jax/requirements.txt https://gist.githubusercontent.com/opencoca/ea36cfdf091c93df337f36951fa93a50/raw/83f7e5246a0c7ed873569e1edab93ef289252f1e/requirements.txt\n",
        "!pip install --target=$pk_path -r mesh-transformer-jax/requirements.txt\n",
        "\n",
        "# jax 0.2.12 is required due to a regression with xmap in 0.2.13\n",
        "!pip install --target=$pk_path mesh-transformer-jax/ jax==0.2.12\n",
        "!pip install --target=$pk_path optax==0.0.9 transformers==4.12.3 dm-haiku==0.0.5 einops==0.3.2 jax==0.2.12 ray==1.8.0 \n",
        "\n",
        "# exit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO1UXepF-0Uq"
      },
      "source": [
        "## Setup Model\n",
        "\n",
        "Things will auto restart your runtime for this will work. \n",
        "\n",
        "If you wan to use the side pannel to explore files click in the top right and connect to hosted runtime. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ex0qJgaueZtJ"
      },
      "source": [
        "#@title Setup JAX for TPU use\n",
        "import os\n",
        "import requests \n",
        "from jax.config import config\n",
        "\n",
        "colab_tpu_addr = os.environ['COLAB_TPU_ADDR'].split(':')[0]\n",
        "url = f'http://{colab_tpu_addr}:8475/requestversion/tpu_driver0.1_dev20210607'\n",
        "print(url)\n",
        "requests.post(url)\n",
        "\n",
        "# The following is required to use TPU Driver as JAX's backend.\n",
        "config.FLAGS.jax_xla_backend = \"tpu_driver\"\n",
        "config.FLAGS.jax_backend_target = \"grpc://\" + os.environ['COLAB_TPU_ADDR']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIgUVdFLe4A8"
      },
      "source": [
        "Sometimes the next step errors for some reason, just run it again ¯\\\\\\_(ツ)\\_/¯"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-A5IGYSaeze3"
      },
      "source": [
        "#@title Import libraries\n",
        "import time\n",
        "\n",
        "import jax\n",
        "from jax.experimental import maps\n",
        "import numpy as np\n",
        "import optax\n",
        "import transformers\n",
        "\n",
        "from mesh_transformer.checkpoint import read_ckpt\n",
        "from mesh_transformer.sampling import nucleaus_sample\n",
        "from mesh_transformer.transformer_shard import CausalTransformer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAgKq-X2kmba",
        "cellView": "form"
      },
      "source": [
        "#@title Set Network Parameters \n",
        "params = {\n",
        "  \"layers\": 28,\n",
        "  \"d_model\": 4096,\n",
        "  \"n_heads\": 16,\n",
        "  \"n_vocab\": 50400,\n",
        "  \"norm\": \"layernorm\",\n",
        "  \"pe\": \"rotary\",\n",
        "  \"pe_rotary_dims\": 64,\n",
        "\n",
        "  \"seq\": 2048,\n",
        "  \"cores_per_replica\": 8,\n",
        "  \"per_replica_batch\": 1,\n",
        "}\n",
        "\n",
        "per_replica_batch = params[\"per_replica_batch\"]\n",
        "cores_per_replica = params[\"cores_per_replica\"]\n",
        "seq = params[\"seq\"]\n",
        "\n",
        "\n",
        "params[\"sampler\"] = nucleaus_sample\n",
        "\n",
        "# here we \"remove\" the optimizer parameters from the model (as we don't need them for inference)\n",
        "params[\"optimizer\"] = optax.scale(0)\n",
        "\n",
        "mesh_shape = (jax.device_count() // cores_per_replica, cores_per_replica)\n",
        "devices = np.array(jax.devices()).reshape(mesh_shape)\n",
        "\n",
        "maps.thread_resources.env = maps.ResourceEnv(maps.Mesh(devices, ('dp', 'mp')))\n",
        "\n",
        "tokenizer = transformers.GPT2TokenizerFast.from_pretrained('gpt2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "T7kJZTkVQ2CZ"
      },
      "source": [
        "#@title Install Tracery \n",
        "#@markdown Tracery is a gramar for spining articles, stories, and text in general. https://github.com/aparrish/pytracery\n",
        "!pip install tracery"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xgsL0IFTZt0"
      },
      "source": [
        "#@title Tidy things up\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwNETD2Uk8nu",
        "outputId": "b314f09a-ef4e-4ed8-d880-1b6671e195f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@title Create the network\n",
        "#@markdown ...loading neural weights from the downloaded files. \n",
        "#@markdown\n",
        "#@markdown  > *This can take around 5 minutes.*\n",
        "total_batch = per_replica_batch * jax.device_count() // cores_per_replica\n",
        "\n",
        "network = CausalTransformer(params)\n",
        "\n",
        "network.state = read_ckpt(network.state, \"step_383500/\", devices.shape[1])\n",
        "\n",
        "network.state = network.move_xmap(network.state, np.zeros(cores_per_replica))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters: 6053381344\n",
            "read from disk/gcs in 112.301s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-eT7Sw6if4J"
      },
      "source": [
        "# Run Model\n",
        "\n",
        "Finally, we are ready to infer with the model! The first sample takes around a minute due to compilation, but after that it should only take about 10 seconds per sample.\n",
        "\n",
        "Feel free to mess with the different sampling parameters (top_p and temp), as well as the length of the generations (gen_len, causes a recompile when initially changed - recompiles will be cached).\n",
        "\n",
        "You can also change other things like per_replica_batch in the previous cells to change how many generations are done in parallel. A larger batch has higher latency but higher throughput when measured in tokens generated/s. This is useful for doing things like best-of-n cherry picking.\n",
        "\n",
        "*Tip for best results: Make sure your prompt does not have any trailing spaces, which tend to confuse the model due to the BPE tokenization used during training.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuZq54dh8opl",
        "cellView": "form"
      },
      "source": [
        "#@markdown We allow for text wrapping of whitespace via css injection.\n",
        "\n",
        "# allow text wrapping in generated output: https://stackoverflow.com/a/61401455\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVzs2TYlvYeX",
        "cellView": "form",
        "outputId": "724b7255-6d6d-4610-e42d-da2b041c299f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "#@title Define our infer function. \n",
        "def infer(context, top_p=0.9, temp=1.0, gen_len=256):\n",
        "    tokens = tokenizer.encode(context)\n",
        "\n",
        "    provided_ctx = len(tokens)\n",
        "    pad_amount = seq - provided_ctx\n",
        "\n",
        "    padded_tokens = np.pad(tokens, ((pad_amount, 0),)).astype(np.uint32)\n",
        "    batched_tokens = np.array([padded_tokens] * total_batch)\n",
        "    length = np.ones(total_batch, dtype=np.uint32) * len(tokens)\n",
        "\n",
        "    start = time.time()\n",
        "    output = network.generate(batched_tokens, length, gen_len, {\"top_p\": np.ones(total_batch) * top_p, \"temp\": np.ones(total_batch) * temp})\n",
        "\n",
        "    samples = []\n",
        "    decoded_tokens = output[1][0]\n",
        "\n",
        "    for o in decoded_tokens[:, :, 0]:\n",
        "      samples.append(f\"\\033[1m{context}\\033[0m{tokenizer.decode(o)}\")\n",
        "\n",
        "    #print(f\"completion done in {time.time() - start:06}s\")\n",
        "    #this should instead be added to the object returned infered.samples and infered.completion_time\n",
        "    return samples\n",
        "\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjWw1zkEOP8z",
        "outputId": "7d3d970f-3021-4419-e163-6d82b44ca8a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#@title Warmup our inference engine \n",
        "#@markdown The first time that infrences are run for a given length takes imensly longer than on repeated runs. For this reason we spin up the inference engine with a few sample runs. With this out of the way we can quickly experiment with our infrence engine. \n",
        "\n",
        "context = \"\"\"\n",
        "# A NEW WAY TO WORK IN THE OLD PORT OF MONTREAL\n",
        "## SPACES CITÉ MULTIMÉDIA\n",
        "\n",
        "The Old Port of Montreal is a\"\"\"\n",
        "\n",
        "for i in [32,64,96,128,256,512,1024,2048]:\n",
        "  print(\"- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\")\n",
        "  print(f\" ## Infering context with a gen_len of {i} this should take ~ {i/32} sec. when run again ##\")\n",
        "  print(\"- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\")\n",
        "  print(infer(context, gen_len=i)[0])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            " ## Infering context with a gen_len of 32 this should take ~ 1.0 sec. when run again ##\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "\u001b[1m\n",
            "# A NEW WAY TO WORK IN THE OLD PORT OF MONTREAL\n",
            "## SPACES CITÉ MULTIMÉDIA\n",
            "\n",
            "The Old Port of Montreal is a\u001b[0m wonderful place for everyone to experience. More than just a concrete waterway, it's a multi-functional space where cultural activities and sports take place. It's\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            " ## Infering context with a gen_len of 64 this should take ~ 2.0 sec. when run again ##\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "\u001b[1m\n",
            "# A NEW WAY TO WORK IN THE OLD PORT OF MONTREAL\n",
            "## SPACES CITÉ MULTIMÉDIA\n",
            "\n",
            "The Old Port of Montreal is a\u001b[0m multifaceted space where rich and poor, young and old live side by side, and where all kinds of artistic practices can take place. This innovative, multidirectional approach will be the core of this collection of \"monumental\" works and projects that took place in the Old Port from 2008 to the present\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            " ## Infering context with a gen_len of 96 this should take ~ 3.0 sec. when run again ##\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "\u001b[1m\n",
            "# A NEW WAY TO WORK IN THE OLD PORT OF MONTREAL\n",
            "## SPACES CITÉ MULTIMÉDIA\n",
            "\n",
            "The Old Port of Montreal is a\u001b[0m vibrant contemporary cultural centre, a true cosmopolitan public space and a space of shared experience for everyone in the city and in the world.\n",
            "\n",
            "Spaces Cité Multimedia focuses on innovative social practices that contribute to the rejuvenation of the city. It explores ways to establish ties with people, as well as ways to engage with visitors. It is about nurturing spaces that can accommodate diverse artistic and creative expressions while allowing residents and visitors to enjoy Montreal's rich historical and artistic heritage and\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            " ## Infering context with a gen_len of 128 this should take ~ 4.0 sec. when run again ##\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "\u001b[1m\n",
            "# A NEW WAY TO WORK IN THE OLD PORT OF MONTREAL\n",
            "## SPACES CITÉ MULTIMÉDIA\n",
            "\n",
            "The Old Port of Montreal is a\u001b[0m place where French, English and everything else meet and mingle. It is a place of quays and warehouses, where goods are brought in, stores are sold, and ships are docked. Today it's a grand public space and the centerpiece of a vibrant new neighborhood. The Old Port has changed so much that when people speak of this city's past, they no longer refer to the Square Dorchester but to the Back Road and the beautiful boulevard that runs along its shores, called Côte des Meuniers.\n",
            "\n",
            "The images in this book are a few of the many facets of the Old Port. They are objects\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            " ## Infering context with a gen_len of 256 this should take ~ 8.0 sec. when run again ##\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "\u001b[1m\n",
            "# A NEW WAY TO WORK IN THE OLD PORT OF MONTREAL\n",
            "## SPACES CITÉ MULTIMÉDIA\n",
            "\n",
            "The Old Port of Montreal is a\u001b[0m centre of arts and culture and, as such, deserves to have its infrastructure and physical appearance updated and renovated for the use of contemporary and future visitors. The main challenge is not to upset the integrity of the historical and cultural fabric of the Old Port but to insert new infrastructures, spaces, streets, buildings and environments that will contribute to the success of the areas to be developed.\n",
            "\n",
            "Rather than make Montreal's main riverwalk the new main street of the city, it should also become a space of pedestrian life for locals and visitors. Reintroducing the railway, like that of the nineteenth century, and introducing subways and other public transport services, such as bicycle rental, could be done at no cost to the City of Montreal or to the province and would certainly contribute to the success of downtown Montreal. By working hand in hand with private enterprise, it is possible to develop private spaces that complement public ones. In addition to the numerous hotels already found in this area, the development of a 15,000 m2 public park to the south of the West Island in collaboration with private owners and the preservation of the edge of the Rivière des Prairies to the north would create a park-like area that would become a true great place of recreation.\n",
            "\n",
            "\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            " ## Infering context with a gen_len of 512 this should take ~ 16.0 sec. when run again ##\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "\u001b[1m\n",
            "# A NEW WAY TO WORK IN THE OLD PORT OF MONTREAL\n",
            "## SPACES CITÉ MULTIMÉDIA\n",
            "\n",
            "The Old Port of Montreal is a\u001b[0m working port built between the 17th and 20th centuries. Located just east of downtown, it was used extensively for the port's primary functions for hundreds of years. However, with the advent of containerization, and the shift from boat-based freight transportation to rail freight, this old port became increasingly obsolete, resulting in its demolition in the 1970s.\n",
            "\n",
            "However, Montrealers weren't willing to just let an integral part of the city's history and culture fall. They reclaimed it and, together with the public and private sectors, decided to turn the new space into a multipurpose public space that would reflect the uniqueness of the city's past, its modernity, and its place in the global economy. The Old Port has been transformed into a place where different types of activities are going on in an attempt to revive the history and structure of the port and to meet the needs of businesses in this new urban context.\n",
            "\n",
            "The numerous businesses that have set up shop in the Old Port, and the spaces that have been created for them to operate, are as varied as the mix of people who inhabit the Old Port and have been the subject of a number of recent studies. They range from traditional boat repair shops and restaurants, to offices, co-working spaces, design shops, art studios, and a large and growing number of local small and medium-sized enterprises (SMEs). The spaces reflect the history of the port and its ongoing renaissance, giving an important second life to this part of the city.\n",
            "\n",
            "# Map 1\n",
            "\n",
            "  * **The Old Port and Jarry Street**\n",
            "  * **Old Montreal**\n",
            "  * **Map 2**\n",
            "\n",
            "  * **Site map**\n",
            "\n",
            "' **Entreprise opérationnelle de négociation directe entre le public et l'entreprise dans le but d'améliorer la légitimité de la législation, de la fiscalité, des services et des ressources des sociétés de privé au Québec'**\n",
            "\n",
            "\"Business operating agency for direct negotiation between the public and the company with the goal of improving the legitimacy of the law, taxation, services and resources of private companies in Quebec\"\n",
            "\n",
            "THE PERFORMERS: The Spaces Cité Multimedia is a combination of different contributors. The Website includes texts written by the municipal government, local businesses, artists, intellectuals, and others. The blog section includes articles by local business people, scholars,\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            " ## Infering context with a gen_len of 1024 this should take ~ 32.0 sec. when run again ##\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "\u001b[1m\n",
            "# A NEW WAY TO WORK IN THE OLD PORT OF MONTREAL\n",
            "## SPACES CITÉ MULTIMÉDIA\n",
            "\n",
            "The Old Port of Montreal is a\u001b[0m world-class example of a successful 21st century urban living environment that is warm, welcoming, and stimulating. Fusing culture, people, and the public realm, it is a microcosm of Montreal itself, replete with its own character, customs, and way of doing things. Located on the banks of the St. Lawrence River, the Old Port has been a crossroads for people and businesses since the 17th century. When its arrival in the mid-20th century, it was initially considered an unfashionable district, and the physical appearance was not especially appealing. Now, in the late 2000s, it has been transformed into a vibrant commercial, cultural, and pedestrian haven with many diverse and unique qualities.\n",
            "\n",
            "**Planned in a unique style,** the O2 Olympic Casino and the adjacent Olympic Stadium form the main focus of Montreal's Old Port, and their presence alone will have an enormous impact on the area. Furthermore, the four skyscrapers of the Olympic Village are rising in the north of the Old Port, while the BioLife complex is coming up in the south, transforming the University of Montreal campus into a new residential, commercial, and educational quarter. This trend of large-scale development within the confines of the Old Port will continue in the years to come, with new projects like the Pointe-à-Callière Museum of Contemporary Art and the Laval Metro station.\n",
            "\n",
            "The new vision of the Old Port of Montreal represents a complete rethink of a place that has always defied categorization. In its totality, it has become the ideal place in which to live, work, and do business, and we have a say in its development. Located on a site of just under 18 hectares, the Old Port is also part of an 8.5-hectare urban park, which together with its 12 acres of urban forest offers a protective cover against climate change and will serve as the natural backbone of the city.\n",
            "\n",
            "### Concept and urban design\n",
            "\n",
            "The symbolic Old Port is not one of the 19th-century mercantile district but rather a \"Porte\" or gate, linked to the city's dynamic network of canals and waterways. Its defining characteristic is a decidedly maritime theme that transcends urban classifications. Its overall configuration is clearly of maritime trade origins, and many of its oldest buildings and homes, the presences that first defined the Old Port and that are its backbone, are water-bound structures dating back to the 17th and 18th centuries.\n",
            "\n",
            "In designing the Old Port, there was no question of creating a wasteland or of annihilating its architecture and heritage; it needed to evolve and grow with the times, recognizing that its character was as much a part of its image as its heritage. Recognizing this, and wanting to maintain and respect its identity, the Port Commission opted for the use of a vernacular vocabulary that blended the innovative. It also decided to continue using brick as the primary building material for this project. As its historic context led to brick being used, the project would also reflect the traditional use of this material. Its identity, created from the sea, would also be reflected in the urban fabric.\n",
            "\n",
            "The transformation of the Old Port had to move away from the grand, perfect-cube model. Thus, the new concept was completely based on a program for public spaces, some of which are very different from the traditional approaches that we are familiar with. The definition of the Old Port in the postindustrialist era was radically shifted. Its place and function as a natural habitat for water was defined in the role of creating a port-like location that is open to the water, an integral part of the environment and to the city, and a source of ecology, protection, and healthy living.\n",
            "\n",
            "The Port Commission's role in this design process was critical. Working in close collaboration with several architectural studios, it decided that the Old Port should be a public space that would draw people from the surrounding areas. Its public location and setting would create a valuable incentive for those who wanted to move in; its physical space would be the defining feature.\n",
            "\n",
            "To find out about the overall strategy, and to begin a dialogue, the Port Commission invited a team of experts in urban ecology to carry out a study of the city and its current infrastructure and infrastructure problems. This was the beginning of a series of workshops, where designers and planners could examine the problem and discuss what to do about it.\n",
            "\n",
            "At the first of these workshops, the idea of the port-like atmosphere became a challenge for the designers and planners to find a new solution for all aspects of the project, beginning with the urban fabric and moving to the new public space, to its relation to the buildings and the workspaces, and to its relation to the existing buildings. The entire transformation of the area had to be examined and designed so that it met the needs of the day and was not an imposition or an imposition to visitors.\n",
            "\n",
            "\"The project is the continuation of my studies in urban ecology... This\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            " ## Infering context with a gen_len of 2048 this should take ~ 64.0 sec. when run again ##\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "\u001b[1m\n",
            "# A NEW WAY TO WORK IN THE OLD PORT OF MONTREAL\n",
            "## SPACES CITÉ MULTIMÉDIA\n",
            "\n",
            "The Old Port of Montreal is a\u001b[0m thriving neighbourhood within Montreal's 17th district, La Petite-Patrie, where workers' bars and restaurants are gathered under the spotlights of Montreal's media industry. In 2005, a media space was launched to allow for the dialogue and exchange of ideas in the interest of supporting, promoting and strengthening the media environment.  \n",
            "... _késai, ésoui_...\n",
            "\n",
            "Ex-libris, Kesta\n",
            "\n",
            "## SPACES CITÉ MULTIMÉDIA\n",
            "\n",
            "**DEBÜSÜK ANGAZIÜR**  \n",
            "Nader Ekic, academic writer\n",
            "\n",
            "**PYÇO**  \n",
            "Sébastien Bois, founder of Friseur du Musée\n",
            "\n",
            "**DÉFI PAR ADÉÜÈSE**  \n",
            "Guy Portelance, an award-winning documentary filmmaker and journalist\n",
            "\n",
            "**J'AI BEAU PARFUMER**  \n",
            "Philippe Aubé, professional perfumer\n",
            "\n",
            "**GÜNCELERIM, GÜNCELERİM VE ARATLARIMIZ**  \n",
            "Orhan Demirkol, film director\n",
            "\n",
            "**SVANDA VE KARLAR**  \n",
            "Aris Tiryaki, anthropologist\n",
            "\n",
            "**DÜŞÜKLI TERBÜRÜNGLER**  \n",
            "Simon Dumeige, design engineer, corporate chef\n",
            "\n",
            "**MEKANLAR**  \n",
            "Pamela Biruk, a multimedia producer\n",
            "\n",
            "**VÖRÜNCELERİN İLK OLMAK İÇİN UÇQUEL OTOÇ**  \n",
            "Kenan Aslan, film director\n",
            "\n",
            "**ETİP ŞAÇ**  \n",
            "Haluk Ateş, author\n",
            "\n",
            "**VARLAKÜLLER**  \n",
            "Caner Taşkın, ethnologist\n",
            "\n",
            "**HİLALIZOĞLU ÖDÜLDÜRÜCELER**  \n",
            "Selçuk Çalış, designer\n",
            "\n",
            "**RAKET MÜZESİ**  \n",
            "Asaf Gozlu, DJ\n",
            "\n",
            "**KURUMSALEM BEYİM KORLAMASÜZ**  \n",
            "Ankara Kaya, geographer\n",
            "\n",
            "## THE SPACES\n",
            "\n",
            "**CHAKLİ SOHRAN**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Hüseyin Atila Kire**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Orhan Er**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Bülent Mutlu**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Izzet Öktem**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Engin Polat**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Şefik Tekin**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Merdane Gündüz**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Sait Sahin Mertens**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Aysel Yalman**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Ahmet Güneş**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Mehmet Kaya**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Cemil Akpınar**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Alpan Mese**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Araki Meral Ünay**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Kadir İnan**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Sadi Kolat**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Zeynep Sabri Özata**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Halid Gökmen**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Nüsret Sezer**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Nilüfer Çevik**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Özgür Ağır**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Ali Başazlı**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Mehmet Hakan Atay**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Onur Koç**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Fatma Koru**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Şebnem Bey**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Zeynep İnce**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Cemil Ertem**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Ümit Kaya**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Pelin Tanbay**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Emre Akbaş**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Ali Ayaz**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Canan Karakoyunlu**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Murat Güler**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Ümit Kaya**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Nail Tokutak**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Ayşe Kadıoğlu**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Yusuf Soyka**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Rikka Benyamin**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Seyit Kenan Şahin**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Özge Acar**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Murat Anayev**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Necdet Yaşar Usta**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Murat Alkan**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Yasemin Altunel**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Çağan Güngördü**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Ümit Kaya**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Zihniyet Alpeyberoğlu**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Ezgi Alman**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Alp Ceviz**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Süleyman Yüce**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Cemal Şenol**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Kader Konyel**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Merve İdil**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Muharrem Kıral**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Ümit Taşyen**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Esra Künay**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Emre Ünsal**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Ümit Kaya**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Onur Koç**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Ümit Kaya**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Zehra Barut**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Kaya Yürükçüler**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Yıldız Eroğlu**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Pamuk Akyürek**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Sevin Üniversitesi**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Güleş Ağır**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Sedat Şenocak**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Pınar Karaca**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Rüstem Tuna**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Lemat Keskin**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Yılmaz Kavas**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Seda Sürtan**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Evin Vancan**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Zeynep Sahil Yavuz**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Can Er**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Günay Kabadayı**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Vera Bellasalma**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Selçuk Çalış**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Meryem Ülker**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Çiğdem Yabacan**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Ertan Yazar**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Muhsin Ertuğrul**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Oğuz Denizkaya**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Rüya M. Tepe**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Burcu Tükel**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Eda Altay**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Ceyda Ağlan**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Necil Şener**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Mehmet Çağlayan**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Muharrem Kıral**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Çevik İdil**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Emre İş**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Ayhan Karakas**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Merve Köksal**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Bülent Mutlu**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Ahmet Günş**\n",
            "\n",
            "**»**\n",
            "\n",
            "**Paçu TeknionĒs inace.**\n",
            "\n",
            "Isabeau\n",
            "\n",
            "R\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mO89-sKMrucR"
      },
      "source": [
        "# Example prompts\n",
        "\n",
        "We've compiled the model. At this point it should only take about 15 seconds per sample. The first time you run inference on a new dataset takes longer than usual because of compilation to build the graph and write data into disk buffers for fast access by CPUs/GPUs.\n",
        "\n",
        "The following parameters can be adjusted: top_p, temp, gen_len. When changing the length of generations (gen_len), recompilation is required to maintain accuracy and consistency in results.\n",
        "\n",
        "You can also change other things like the number of generations done in parallel and how many samples are generated. In addition, you can increase or decrease the batch size for latency/throughput tradeoffs as well. This is useful when performing best-of-n cherry picking tasks where throughput needs to be maximized with minimum batch time delay on each sample generation process.\n",
        "\n",
        "To get the best results, make sure your prompt does not have any trailing spaces. This is because BPE tokenization can be confused by them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-ViSfFkOUbS"
      },
      "source": [
        "## Simple Inference "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EStwU8WqXnBD"
      },
      "source": [
        "### Set Sampling Parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akM-tZFTXnBE",
        "cellView": "form"
      },
      "source": [
        "#@markdown ### Leave this alone unless you know what you are doing! \n",
        "#@markdown ... or you are just having fun experimenting and don't mind having weird results.\n",
        "#@markdown - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
        "#@markdown Top_p controls diversity/quality. The higher our top_p the more rich and nuanced oir inferences can be.\n",
        "top_p = 0.9 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "#@markdown - - - \n",
        "#@markdown Temp controls randomnes. Low temps make for mechanical inferences.\n",
        "temp = 0.9 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "#@markdown - - - \n",
        "#@markdown Gen_len adjects the length of text infered.\n",
        "#@markdown Specifcily it adustes the nember of tokens (~4 per engish word).\n",
        "#@markdown\n",
        "#@markdown *Note: Changing gen_len will cause long wait times when first setting lengs that have not been set before.*\n",
        "#@markdown  \n",
        "gen_len = 64 #@param {type: \"slider\", min:64, max: 1024, step: 64}\n",
        "#@markdown - - - \n",
        "#@markdown Generate controls the number of inferences that will be generated.\n",
        "generate = 1 #@param {type: \"slider\", min:1, max: 12, step: 1}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wUL-ryfYueS"
      },
      "source": [
        "### Text examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvlAK6RbCJYg",
        "cellView": "form"
      },
      "source": [
        "#@title Raw Context \n",
        "#@markdown This is an example of simple summery infrence.\n",
        "\n",
        "#print(\"Enter/Paste your content. Ctrl-D or Ctrl-Z ( windows ) to save it.\")\n",
        "#context = []\n",
        "#while True:\n",
        "#    try:\n",
        "#        line = input()\n",
        "#    except EOFError:\n",
        "#        break\n",
        "#    context.append(line)\n",
        "\n",
        "context = \"\"\"\"\n",
        "\n",
        "Article:\n",
        "\n",
        "t’s an intense crypto dogfight as dogecoin (DOGE) and shiba inu (SHIB), two of the most popular meme coins, battle it out for the ninth spot on the list of top digital assets by market capitalization. Some traders are profiting from the action by taking spread trades.\n",
        "\n",
        "DOGE has come alive after lagging SHIB by a significant margin earlier this month. The joke cryptocurrency surged to $0.335 on Coinbase on Thursday, hitting the highest level since Aug. 20. It was last trading near $0.30, representing a 22% gain on the day.\n",
        "\n",
        "Meanwhile, SHIB crashed as much as 30% to $0.00006 earlier today, having chalked out a rally to $0.00009 in the seven days to Oct. 27.\n",
        "\n",
        "Summery:\"\"\"\n",
        "\n",
        "print(infer(top_p=top_p, temp=1, gen_len=96, context=context)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2omRWxDivro",
        "cellView": "form"
      },
      "source": [
        "#@title Expand into a script \n",
        "#@markdown The will expand a lead into a script bassed on the opening of pulp fiction.\n",
        "write_script_on_this = \"Feline biology is amazing\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown - - - - - - - - - - - - - - - - - - - - - - - - -\n",
        "#@markdown These options don't need to be adjusted.\n",
        "top_p = 0.9 #@ param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "temp = 0.9 #@ param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "gen_len = 512 #@param {type: \"slider\", min:64, max: 2048, step: 64}\n",
        "generate = 1 #@ param {type: \"slider\", min:1, max: 12, step: 1}\n",
        "#@markdown - - - - - - - - - - - - - - - - - - - - - - - - -\n",
        "\n",
        "\n",
        "context = \"\"\"\n",
        "\n",
        "INT. COMPUTER LAB – MORNING\n",
        "\n",
        "A normal computer lab, an office or university, Toronto Canada.\n",
        "It's about 9:00 in the morning. While the place isn't jammed,\n",
        "there's a healthy number of people working, drinking coffee, \n",
        "reading at their computers.\n",
        "\n",
        "Two of these people are a YOUNG MAN and a YOUNG WOMAN. The\n",
        "Young Man has a slight working-class English accent.\n",
        "\n",
        "             YOUNG MAN\n",
        "  I want to tell you about what we've been studying.\n",
        "\n",
        "             YOUNG WOMAN\n",
        "  Oh? What have you been focusing on? \n",
        "\n",
        "             YOUNG MAN\n",
        "  {expand_this}. For example\"\"\"\n",
        "\n",
        "\n",
        "for x in range(generate):\n",
        "  try:\n",
        "    print(infer(top_p=top_p, \n",
        "                temp=temp, \n",
        "                gen_len=gen_len, \n",
        "                context=context.format(expand_this = expand_this))[0])\n",
        "  except:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eN8f4mvA97nG",
        "cellView": "form"
      },
      "source": [
        "#@title Make a title sound more academic \n",
        "#@markdown  \n",
        "improve_this = \"we're going ti make an instrument using the potentiometer \" #@param {type:\"string\"}\n",
        "#@markdown - - -\n",
        "top_p = 0.9 #@ param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "temp = 0.9 #@ param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "gen_len = 64 #@ param {type: \"slider\", min:64, max: 2048, step: 64}\n",
        "generate = 12 # @ param {type: \"slider\", min:1, max: 12, step: 1}\n",
        "\n",
        "context = \"\"\"\n",
        "Instead of saying, \"{improve_this}\" a better shibboleth would be, \\\"\"\"\"\n",
        "\n",
        "#Indie Hackers want to click on titles such as \n",
        "#use something with more punch\n",
        "\n",
        "for x in range(generate):\n",
        "  try:\n",
        "    print(infer(top_p=top_p, \n",
        "                temp=temp, \n",
        "                gen_len=gen_len, \n",
        "                context=context.format(improve_this = improve_this))[0].split('\"')[3])\n",
        "  except:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vf63EvRmipH2"
      },
      "source": [
        "#@title Give headlines some more Clickbait \n",
        "improve_this = \"Make an instrument using the a light sensor \" #@param {type:\"string\"}\n",
        "#@markdown - - - - - - - - - - - - - - - - - - - - - - - - \n",
        "top_p = 0.9 #@ param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "temp = 1 #@ param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "gen_len = 64 # @ param {type: \"slider\", min:64, max: 2048, step: 64}\n",
        "generate = 12 #@ param {type: \"slider\", min:1, max: 12, step: 1}\n",
        "\n",
        "context = \"\"\"\n",
        "You need to make your title more clickbate. So instead of,\n",
        "\"{improve_this}\" use something with more punch such as,\\\"\"\"\"\n",
        "\n",
        "\n",
        "for x in range(generate):\n",
        "  try:\n",
        "    print(infer(top_p=top_p, \n",
        "                temp=temp, \n",
        "                gen_len=gen_len, \n",
        "                context=context.format(improve_this = improve_this))[0].split('\"')[3])\n",
        "                #context=context.format(improve_this = improve_this))[0])\n",
        "\n",
        "  except:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIX6EZWOlDGA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 739
        },
        "id": "rCjGKMEJlDjJ",
        "outputId": "016bfc79-a0c2-401b-fe5d-7aaa3bae8660"
      },
      "source": [
        "# @ title Generating inference prompts using tracery\n",
        "import tracery\n",
        "from tracery.modifiers import base_english\n",
        "\n",
        "rules = {\n",
        "    \n",
        "    'origin': 'It was a #type_of_day# #time# in #location# and',\n",
        "    'time': ['night', 'day', 'week', 'year'],\n",
        "    'type_of_day': ['amazing', 'spectaculare' , 'horid', 'spooky' ],\n",
        "    'location': ['Montreal', 'the woods', 'my head']\n",
        "}\n",
        "\n",
        "grammar = tracery.Grammar(rules)\n",
        "grammar.add_modifiers(base_english)\n",
        "\n",
        "for x in range(generate):\n",
        "  try:\n",
        "    print(infer(top_p=top_p, \n",
        "                temp=temp, \n",
        "                gen_len=gen_len, \n",
        "                context=grammar.flatten(\"#origin#\"))[0] + \"\\n\")\n",
        "  except:\n",
        "    pass"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mIt was a horid year in the woods and\u001b[0m the good folks of the hamlet of\n",
            "Breckenridge did not know what to do about it, for the wolves had come\n",
            "again and a good-sized herd of elk was grazing close by, and if the\n",
            "people didn’t interfere it would be all right; so they kept quiet and\n",
            "passed the time by exchanging tales of the old days and the troubles\n",
            "that they had suffered.\n",
            "\n",
            "There was a goodly fat buck, too, that had just been turned out of the\n",
            "lodge and was sleeping off his meal. One morning early he heard some\n",
            "one moving and woke up to find a little bit of a chap standing close\n",
            "beside him and peeping in his face.\n",
            "\n",
            "“Where are you,” asked the buck, “and what are you doing there?”\n",
            "\n",
            "“Oh,” said the little chap, “I was looking for some water. My father\n",
            "told me to go a little way down the creek and look for some.”\n",
            "\n",
            "“Well,” said the buck, “you have found a good place for a drink,\n",
            "and I’m not sure that your father is right. The wolves are prowling\n",
            "about, and they would get you, and besides, you’ll get the fidgets if\n",
            "you stay here.”\n",
            "\n",
            "“Oh,” said the little chap, “that won’t matter, because when I have a\n",
            "good drink I sleep very sound.”\n",
            "\n",
            "“Well,” said the buck, “I don’t care whether you do or not, so long as\n",
            "you get away from here. You’re all right, but there are other places\n",
            "where I could put you.”\n",
            "\n",
            "“Do you mean that if I follow you there will be wolves prowling about\n",
            "too?”\n",
            "\n",
            "“Yes,” said the buck, “but you’ll get enough of them there without\n",
            "stopping to watch them.”\n",
            "\n",
            "“All right,” said the little chap, “I’ll follow you.”\n",
            "\n",
            "They went down the creek and there was a wolf close at their heels and\n",
            "his teeth were clattering and his eyes were dancing with excitement.\n",
            "\n",
            "The little chap wanted to ask the buck what he was going to do about\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvek96CyOnOh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2a052f22-826e-44d4-8498-ecb85dd972ac"
      },
      "source": [
        "# @ title Generating inference prompts using tracery\n",
        "import tracery\n",
        "from tracery.modifiers import base_english\n",
        "\n",
        "rules = {\n",
        "    \n",
        "    'origin': 'Our team builds #imagine# new online #location# beyond',\n",
        "    'imagine': ['amazing', 'spectaculare' , 'bold', ],\n",
        "    'location': ['worlds', 'spaces', 'colabortion']\n",
        "}\n",
        "\n",
        "grammar = tracery.Grammar(rules)\n",
        "grammar.add_modifiers(base_english)\n",
        "\n",
        "for x in range(generate):\n",
        "  try:\n",
        "    print(infer(top_p=top_p, \n",
        "                temp=temp, \n",
        "                gen_len=gen_len, \n",
        "                context=grammar.flatten(\"#origin#\"))[0] + \"\\n\")\n",
        "  except:\n",
        "    pass"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mOur team builds bold new online spaces beyond\u001b[0m conventional media. By seeking the right opportunities in both traditional and new digital media, we create an immersive online experience for our clients.\n",
            "\n",
            "Journalism\n",
            "\n",
            "Investigating the topics that affect our world\n",
            "\n",
            "Write, research, publish.\n",
            "\n",
            "Here at Spottiswoode, we're all about the new journalism. In this digital age, we produce online journalism with the same expertise, rigour, and substance as our print counterparts.\n",
            "\n",
            "Let us tell your story. Our team of award-winning journalists use the web to show the world the under-reported stories that matter to their audience. From print to broadcast, and online to in-depth investigations, our expertise spans across the media landscape.\n",
            "\n",
            "Media\n",
            "\n",
            "People will read and watch anything.\n",
            "\n",
            "We're a diverse, open and integrated team that works on both new and traditional media.\n",
            "\n",
            "We are experts at combining the digital and print media and digital and broadcast, making sure our clients' content reaches the widest audience. We can run multi-platform campaigns, special media projects, and run radio and TV channels.\n",
            "\n",
            "Media Experts\n",
            "\n",
            "Web Design\n",
            "\n",
            "Websites that work for your business\n",
            "\n",
            "It's not enough to have a great website, we need to have a site that converts visitors into new customers. Our team of experts work to ensure each website has the most modern, effective, and usable design for your business.\n",
            "\n",
            "Media & Events\n",
            "\n",
            "Events that make a difference\n",
            "\n",
            "There's nothing like a good old-fashioned media event to get the world talking. At Spottiswoode, we have the contacts to get you the airtime you want and the publicity you need. We'll plan your event, source the right media, and make sure your message is heard.\n",
            "\n",
            "TV News\n",
            "\n",
            "TV News that matters\n",
            "\n",
            "Our TV news team is an internationally recognised one-stop-shop for business-to-business news. We get the story first, and pitch our clients' business into some of the biggest platforms in the world.\n",
            "\n",
            "Email\n",
            "\n",
            "Email that matters\n",
            "\n",
            "Make sure your company's message is heard. Make sure that people reach you with important information. That's what our email marketing experts do. Whether it's an automated marketing program or a targeted, bespoke campaign, Spottiswoode's email marketing team will make sure your messages are read.\n",
            "\n",
            "Spottiswoode is home to the authors and publishers behind some of the most respected media brands in\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5cJmv5ALxaM",
        "cellView": "form"
      },
      "source": [
        "#@title Generating grading comment inference prompts using tracery\n",
        "import tracery\n",
        "from tracery.modifiers import base_english\n",
        "\n",
        "rules = {\n",
        "    \n",
        "    'origin': '#this_semester# Jane #does# #good_bad# in class. We hope that she',\n",
        "    'this_semester': ['Our focus this semester was learning to program microcomputers, using sensors, and building electronic circuits.','This semester we focused on learning to build electric circuits, reading information from sensors, and programming microcontrollers.'],\n",
        "    'does': ['works', 'does' , 'particpates', 'performs' ],\n",
        "    'good_bad': ['well', 'competently', 'baddly']\n",
        "}\n",
        "\n",
        "grammar = tracery.Grammar(rules)\n",
        "grammar.add_modifiers(base_english)\n",
        "\n",
        "for x in range(generate):\n",
        "  try:\n",
        "    print(infer(top_p=top_p, \n",
        "                temp=temp, \n",
        "                gen_len=gen_len, \n",
        "                context=grammar.flatten(\"#origin#\"))[0] + \"\\n\")\n",
        "  except:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c9FzqkVcZyZ",
        "cellView": "form"
      },
      "source": [
        "#@title Article Generator \n",
        "title = \"Being your own Boss\" #@param {type:\"string\"}\n",
        "site = \"\" #@param {type:\"string\"}\n",
        "date = \"\" #@param {type:\"string\"}\n",
        "author = \"Jim Rohn\" #@param {type:\"string\"}\n",
        "tags = \"goals, drive, gtd\" #@param {type:\"string\"}\n",
        "lead = \"## Table of Content\" #@param {type:\"string\"}\n",
        "#@markdown - - -\n",
        "\n",
        "context = \"\"\"\n",
        "# {title}\n",
        "### Published:{date}\n",
        "### By:{author}\n",
        "##### Tags: {tags}\n",
        "### Site: {site}\n",
        "\n",
        "{lead}\n",
        "\"\"\"\n",
        "\n",
        "for x in range(generate):\n",
        "  print(infer(top_p=top_p, \n",
        "              temp=temp, \n",
        "              gen_len=gen_len, \n",
        "              context=context.format(title =title, \n",
        "                                     site = site, \n",
        "                                     date = date, \n",
        "                                     author = author, \n",
        "                                     tags = tags, \n",
        "                                     lead = lead))[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "nKpUf-M554f9"
      },
      "source": [
        "#@title Article generation { form-width: \"300px\" }\n",
        "title = \"COVID-19: What do vaccine passports in Quebec & Ontario mean for Canada's largest provinces? \" #@param {type:\"string\"}\n",
        "site = \"https://www.canada.ca/en/public-health/services/diseases/\" #@param {type:\"string\"}\n",
        "date = \"Sep 7, 2021\" #@param {type:\"string\"}\n",
        "author = \"\" #@param {type:\"string\"}\n",
        "tags = \"COVID-19, Quebec, Ontario, Canada\" #@param {type:\"string\"}\n",
        "lead = \"The passports are certificates that confirm vaccinations and allow people to enjoy eating\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "#print(\"Enter/Paste your content. Ctrl-D or Ctrl-Z ( windows ) to save it.\")\n",
        "#context = []\n",
        "#while True:\n",
        "#    try:\n",
        "#        line = input()\n",
        "#    except EOFError:\n",
        "#        break\n",
        "#    context.append(line)\n",
        "\n",
        "context = \"\"\"\n",
        "# {title}\n",
        "### Published:{date}\n",
        "### By:{author}\n",
        "##### Tags: {tags}\n",
        "### Site: {site}\n",
        "\n",
        "{lead}\"\"\"\n",
        "\n",
        "for x in range(generate):\n",
        "  print(infer(top_p=top_p, \n",
        "              temp=temp, \n",
        "              gen_len=gen_len, \n",
        "              context=context.format(title =title, \n",
        "                                     site = site, \n",
        "                                     date = date, \n",
        "                                     author = author, \n",
        "                                     tags = tags, \n",
        "                                     lead = lead))[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka8vrZ-kN1Qi"
      },
      "source": [
        "## Code Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiXYadumWbT9"
      },
      "source": [
        "### Set Sampling Parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSKHAVCYPt2W"
      },
      "source": [
        "#@markdown ### Leave this alone unless you know what you are doing ... or are just having fun experimenting.\n",
        "#@markdown \n",
        "#@markdown - - - \n",
        "#@markdown Top_p controls diversity/quality. The higher our top_p the more rich and nuanced oir inferences can be.\n",
        "top_p = 0.9 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "#@markdown - - - \n",
        "#@markdown Temp controls randomnes. Low temps make for mechanical inferences.\n",
        "temp = 0.9 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "#@markdown - - - \n",
        "#@markdown Gen_len adjects the length of text infered.\n",
        "#@markdown Specifcily it adustes the nember of tokens (~4 per engish word).\n",
        "#@markdown\n",
        "#@markdown *Note: Changing gen_len will cause long wait times when first setting lengs that have not been set before.*\n",
        "#@markdown  \n",
        "gen_len = 512 #@param {type: \"slider\", min:64, max: 1024, step: 64}\n",
        "#@markdown - - - \n",
        "#@markdown Generate controls the number of inferences that will be generated.\n",
        "generate = 1 #@param {type: \"slider\", min:1, max: 12, step: 1}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIGNc1D7XBQ-"
      },
      "source": [
        "### Python Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "phNf7ZPj3I8U"
      },
      "source": [
        "#@title Python Code - Simple \n",
        "#@markdown Generate code by just writng a comment and a name for a fuction \n",
        "python_comment = \"check if a number is prime or not\" #@param {type:\"string\"}\n",
        "#variable_n_val = \"youtube_video_url\" #@param {type:\"string\"}\n",
        "function_name = \"p\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "context = \"\"\"# {python_comment}\n",
        "\n",
        "def is_\"\"\".format(python_comment = python_comment, \n",
        "           function_name = function_name)\n",
        "\n",
        "for x in range(generate):\n",
        "  print(infer(top_p=top_p, temp=temp, gen_len=gen_len, context=context)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Gs97bc_DCNIY"
      },
      "source": [
        "#@title Python Code - Complex\n",
        "python_comment = \"detect security flaws \" #@param {type:\"string\"}\n",
        "variable_name =\"person_real_name\" #@param {type:\"string\"}\n",
        "variable_val = \"Isabelle Plante\" #@param {type:\"string\"}\n",
        "function_name = \"online_account_scan\" #@param {type:\"string\"}\n",
        "#@markdown - - -\n",
        "\n",
        "\n",
        "context = '''#!/usr/bin/env python3\n",
        "\n",
        "# {python_comment}\n",
        "\n",
        "{variable_name} = \"{variable_val}\"\n",
        "\n",
        "def {function_name}(n):\n",
        "  \"\"\"This'''.format(python_comment = python_comment,\n",
        "           variable_name = variable_name,\n",
        "           variable_val = variable_val,\n",
        "           function_name = function_name)\n",
        "\n",
        "for x in range(generate):\n",
        "  print(infer(top_p=top_p, temp=temp, gen_len=gen_len, context=context)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2u9WMX9Q-lG"
      },
      "source": [
        "#@title Vue Code - Simple { form-width: \"500px\" }\n",
        "comment = \"guest speakers\" #@param {type:\"string\"}\n",
        "#variable_n_val = \"youtube_video_url\" #@param {type:\"string\"}\n",
        "name = \"tab_1\" #@param {type:\"string\"}\n",
        "#@markdown - - - \n",
        "#@markdown Leave this alone unless you know ...\n",
        "top_p = 0.8 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "temp = 0.9 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "gen_len = 512 #@param {type:\"slider\", min:64, max:1024, step:64}\n",
        "generate = 1 #@param {type: \"slider\", min:1, max: 12, step: 1}\n",
        "\n",
        "context = \"\"\"\n",
        "<!-- Vue template for {comment} page -->\n",
        "<section>\n",
        "  <div id=\"app\" class=\"container\">\n",
        "    <div class=\"content\">\n",
        "      <div id=\"{name} \" class=\"\"\".format(comment = comment, \n",
        "           name = name)\n",
        "      \n",
        "print(infer(top_p=top_p, temp=temp, gen_len=gen_len, context=context)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vb8SkRpDm0rP",
        "outputId": "0a3b37dc-a5a0-44b7-ef4a-f5167f507fb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 655
        }
      },
      "source": [
        "#@title Json Code - Simple { form-width: \"500px\" }\n",
        "title = \"guest_speakers\" #@param {type:\"string\"}\n",
        "#variable_n_val = \"youtube_video_url\" #@param {type:\"string\"}\n",
        "name = \"last_name\" #@param {type:\"string\"}\n",
        "#@markdown - - - \n",
        "#@markdown Leave this alone unless you know ...\n",
        "top_p = 0.9 # @ param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "temp = 0.9 # @ param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "gen_len = 512 # @ param {type:\"slider\", min:64, max:1024, step:64}\n",
        "generate = 1 # @ param {type: \"slider\", min:1, max: 12, step: 1}\n",
        "\n",
        "context = \"\"\"{{\"{title}\": [\n",
        "  {{\"{name}\":\"\"\".format(title = title , name = name)\n",
        "      \n",
        "print(infer(top_p=top_p, temp=temp, gen_len=gen_len, context=context)[0])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m{\"guest_speakers\": [\n",
            "  {\"last_name\":\u001b[0m \"Goodman\",\n",
            "   \"gender\": \"Male\",\n",
            "   \"company\": \"Impatient Soundsystem\",\n",
            "   \"first_name\": \"Adam\",\n",
            "   \"linkedin\": \"\",\n",
            "   \"avatar\": \"https://s-static.ak.fbcdn.net/rsrc.php/v2/yH/r/yGzQeglLBJHM.gif\"\n",
            "  },\n",
            "  {\"last_name\": \"Weinberg\",\n",
            "   \"gender\": \"Male\",\n",
            "   \"company\": \"Solo Cafe\",\n",
            "   \"first_name\": \"Ben\",\n",
            "   \"linkedin\": \"\",\n",
            "   \"avatar\": \"https://s-static.ak.fbcdn.net/rsrc.php/v2/yD/r/yDvnDdJbI5_2E.gif\"\n",
            "  }\n",
            " ],\n",
            " \"guest_categories\": [\n",
            "  {\"id\": \"4d6f2f5d6b2b21f30ed7f8e02f7e867f\",\n",
            "   \"guest_category_type\": \"Organization\",\n",
            "   \"name\": \"CAFE Co-Op\",\n",
            "   \"links\": [\n",
            "    {\"source\": \"http://www.facebook.com/CafeCoOp\",\n",
            "     \"rel\": \"homepage\",\n",
            "     \"type\": \"application/x-amz-url\"\n",
            "    },\n",
            "    {\"source\": \"http://www.facebook.com/CafeCoOp#!/cafes\",\n",
            "     \"rel\": \"places\",\n",
            "     \"type\": \"application/x-amz-url\"\n",
            "    },\n",
            "    {\"source\": \"https://www.facebook.com/CafeCoOp\",\n",
            "     \"rel\": \"network\",\n",
            "     \"type\": \"application/x-amz-url\"\n",
            "    },\n",
            "    {\"source\": \"https://www.facebook.com/cafecoop\",\n",
            "     \"rel\": \"place\",\n",
            "     \"type\": \"application/x-amz-url\"\n",
            "    },\n",
            "    {\"source\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xiwQxqCNdz9"
      },
      "source": [
        "# Examples of GPT genereated code.\n",
        "\n",
        "The following are examples of code generated in prior runs.\n",
        "\n",
        "Please paste interesting examples below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgb9xlP394_5"
      },
      "source": [
        "# check if a number is prime or not\n",
        "\n",
        "def is_prime(n):\n",
        "  \"\"\" \n",
        "  > returns true if n is a prime,\n",
        "  > false otherwise.\n",
        "  \"\"\"\n",
        "  for i in range(2, n):\n",
        "    if n % i == 0:\n",
        "      return False\n",
        "  return True\n",
        "\n",
        "print(is_prime(7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKo_2bvagu2K"
      },
      "source": [
        "# Program to check if a number is prime or not\n",
        "\n",
        "num = 9\n",
        "\n",
        "# Program to check if a number is prime or not\n",
        "\n",
        "def is_prime(n):\n",
        "    while n > 1:\n",
        "        while n % 2 == 0:\n",
        "            return False\n",
        "        n = n / 2\n",
        "    return True\n",
        "\n",
        "print(\"This is an example of a function that works but is quite wrong!\")\n",
        "\n",
        "for i in range(2, 20):\n",
        "  print(f\"Is {i} prime? {is_prime(i)}\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOwWzOgflhW0"
      },
      "source": [
        "# check if a number is prime or not\n",
        "def is_prime(n):\n",
        "    for i in range(2, n):\n",
        "        if n % i == 0:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "is_prime(9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TA5qvTVnS0q8"
      },
      "source": [
        "!pip install grip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXng0F4dSi4i"
      },
      "source": [
        "!pip install grip\n",
        "from google.colab.output import eval_js\n",
        "print(eval_js(\"google.colab.kernel.proxyPort(8000)\"))\n",
        "!grip 8000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Shgf_1CZSkI1"
      },
      "source": [
        "!pip install flask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y43hIJmCirQm"
      },
      "source": [
        "!pip freeze"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}