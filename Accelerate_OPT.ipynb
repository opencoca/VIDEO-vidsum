{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/opencoca/vidsum/blob/master/Accelerate_OPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0Zf_d5RhVpO"
      },
      "source": [
        "# Running OPT up to 30B using `accelerate`\n",
        "\n",
        "This notebook shows how to leverage the dispatching utility in colab, to load even very large checkpoints.\n",
        "\n",
        "This should handle up to 11B in Colab Free, and 30B in colab Pro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R94R2YaeQMQS"
      },
      "outputs": [],
      "source": [
        "! pip install transformers accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EP3YqajghmZC"
      },
      "source": [
        "This downloads the checkpoint. Several checkpoints are available:\n",
        "\n",
        "- [facebook/opt-125m](https://huggingface.co/facebook/opt-125m)\n",
        "- [facebook/opt-350m](https://huggingface.co/facebook/opt-350m)\n",
        "- [facebook/opt-1.3b](https://huggingface.co/facebook/opt-1.3b)\n",
        "- [facebook/opt-2.7b](https://huggingface.co/facebook/opt-2.7b)\n",
        "- [facebook/opt-6.7b](https://huggingface.co/facebook/opt-6.7b)\n",
        "- [facebook/opt-13b](https://huggingface.co/facebook/opt-13b)\n",
        "- [facebook/opt-30b](https://huggingface.co/facebook/opt-30b)\n",
        "\n",
        "It downloads it to cache and we save the link to be re-used afterwards,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35,
          "referenced_widgets": [
            "63d00a78b59c4099b451a2201b998fb4"
          ]
        },
        "id": "XUlpcU3iQNhu",
        "outputId": "00c431d8-c493-4991-cdcb-14bf74c6c642"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/9.87G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "63d00a78b59c4099b451a2201b998fb4"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "checkpoint = 'facebook/opt-30b'\n",
        "weights_path = snapshot_download(checkpoint)\n",
        "\n",
        "# If the folder contains a checkpoint that isn't sharded, it needs to point to the state dict directly\n",
        "# otherwise point to the directory containing the shard\n",
        "import os\n",
        "files = os.listdir(weights_path)\n",
        "weights_path = os.path.join(weights_path, 'pytorch_model.bin') if 'pytorch_model.bin' in files else weights_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWdYkXxblJLh"
      },
      "source": [
        "We then instantiate a configuration, and we load the model from the config inside the `init_empty_weights` decorator. \n",
        "\n",
        "This decorate instantiates an empty shell with the model. This does not actually load or instantiate any weight, only the shapes.\n",
        "\n",
        "This unties the weights, so we manually retie the weights afterwards."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8Ne7jJdaF9F"
      },
      "outputs": [],
      "source": [
        "from accelerate import init_empty_weights, dispatch_model, infer_auto_device_map, load_checkpoint_and_dispatch\n",
        "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "config = AutoConfig.from_pretrained(checkpoint)\n",
        "\n",
        "# Initializes an empty shell with the model. This is instant and does not take any RAM.\n",
        "with init_empty_weights():\n",
        "    model = AutoModelForCausalLM.from_config(config)\n",
        "# Initialize the model under the previous context manager breaks the tied weights.\n",
        "model.tie_weights()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4vNLf02lbKZ"
      },
      "source": [
        "Finally, we infer an a device map automatically from the model. It will place all the layers to disk, CPU RAM and GPU ram according to the available memory in each device."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ltXPR7-izUX"
      },
      "outputs": [],
      "source": [
        "# Infer device map automatically\n",
        "device_map = infer_auto_device_map(model.model, no_split_module_classes=[\"OPTDecoderLayer\"], dtype='float16')\n",
        "\n",
        "if any([k == 'disk' for k in device_map.values()]):\n",
        "    offload_folder = 'offload_folder'\n",
        "else:\n",
        "    offload_folder = None\n",
        "\n",
        "if '30b' in checkpoint:\n",
        "    # Set a few layers to use the disk manually to ensure enough RAM for the 30B checkpoint.\n",
        "    device_map['decoder.layers.23'] = 'disk'\n",
        "    device_map['decoder.layers.24'] = 'disk'\n",
        "    device_map['decoder.layers.25'] = 'disk'\n",
        "    device_map['decoder.layers.26'] = 'disk'\n",
        "    device_map['decoder.layers.27'] = 'disk'\n",
        "\n",
        "device_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OujQRPal_jJ"
      },
      "source": [
        "We load the checkpoint that is saved on disk and we dispatch it to the devices. At no point is the checkpoint fully loaded in RAM; only parts of it to be dispatched to each device.\n",
        "\n",
        "We load it as float16 so that we may load more layers at a time on each device for a faster execution time.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZeA_LQJ3cGbL"
      },
      "outputs": [],
      "source": [
        "load_checkpoint_and_dispatch(\n",
        "    model.model, \n",
        "    weights_path, \n",
        "    device_map=device_map, \n",
        "    offload_folder=offload_folder, \n",
        "    dtype='float16', \n",
        "    offload_state_dict=True\n",
        ")\n",
        "model.tie_weights()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoGXqv_lmVhN"
      },
      "source": [
        "Finally, we create a prompt to generate from and we generate a text from it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ysaqGruhbf2"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('facebook/opt-30b')\n",
        "inputs = tokenizer(\"Hugging Face is pushing the convention that a unicorn with two horns becomes a llama.\", return_tensors=\"pt\")\n",
        "\n",
        "output = model.generate(inputs[\"input_ids\"].to(0), max_length=10, do_sample=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGS9OW5qnaUH"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.decode(output[0].tolist()))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Accelerate_OPT.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}